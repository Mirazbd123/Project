{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225de38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.layers import Dense,LeakyReLU , BatchNormalization\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5460fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\\\dekstop moved ap\\\\Data set\\\\customer churn\\\\Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46620e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0       2       0.00              1          1               1   \n",
      "1       1   83807.86              1          0               1   \n",
      "2       8  159660.80              3          1               0   \n",
      "3       1       0.00              2          0               0   \n",
      "4       2  125510.82              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n",
      "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
      "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
      "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
      "9997       9998    15584532        Liu          709    France  Female   36   \n",
      "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
      "9999      10000    15628319     Walker          792    France  Female   28   \n",
      "\n",
      "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "9995       5       0.00              2          1               0   \n",
      "9996      10   57369.61              1          1               1   \n",
      "9997       7       0.00              1          0               1   \n",
      "9998       3   75075.31              2          1               0   \n",
      "9999       4  130142.79              1          1               0   \n",
      "\n",
      "      EstimatedSalary  Exited  \n",
      "9995         96270.64       0  \n",
      "9996        101699.77       0  \n",
      "9997         42085.58       1  \n",
      "9998         92888.52       1  \n",
      "9999         38190.78       0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3c2432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
       "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
       "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce7f910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"RowNumber\",\"CustomerId\",\"Surname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79efd323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  \n",
       "1          0               1        112542.58       0  \n",
       "2          1               0        113931.57       1  \n",
       "3          0               0         93826.63       0  \n",
       "4          1               1         79084.10       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a1c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender\n",
      "Male      5457\n",
      "Female    4543\n",
      "Name: count, dtype: int64\n",
      "Geography\n",
      "France     5014\n",
      "Germany    2509\n",
      "Spain      2477\n",
      "Name: count, dtype: int64\n",
      "Exited\n",
      "0    7963\n",
      "1    2037\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Gender\"].value_counts())\n",
    "print(df[\"Geography\"].value_counts())\n",
    "print(df[\"Exited\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ecf1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "354c3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,columns=[\"Geography\",\"Gender\"] ,dtype=int, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b5c2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>771</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>516</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>709</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>772</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>792</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0             619   42       2       0.00              1          1   \n",
       "1             608   41       1   83807.86              1          0   \n",
       "2             502   42       8  159660.80              3          1   \n",
       "3             699   39       1       0.00              2          0   \n",
       "4             850   43       2  125510.82              1          1   \n",
       "...           ...  ...     ...        ...            ...        ...   \n",
       "9995          771   39       5       0.00              2          1   \n",
       "9996          516   35      10   57369.61              1          1   \n",
       "9997          709   36       7       0.00              1          0   \n",
       "9998          772   42       3   75075.31              2          1   \n",
       "9999          792   28       4  130142.79              1          1   \n",
       "\n",
       "      IsActiveMember  EstimatedSalary  Exited  Geography_Germany  \\\n",
       "0                  1        101348.88       1                  0   \n",
       "1                  1        112542.58       0                  0   \n",
       "2                  0        113931.57       1                  0   \n",
       "3                  0         93826.63       0                  0   \n",
       "4                  1         79084.10       0                  0   \n",
       "...              ...              ...     ...                ...   \n",
       "9995               0         96270.64       0                  0   \n",
       "9996               1        101699.77       0                  0   \n",
       "9997               1         42085.58       1                  0   \n",
       "9998               0         92888.52       1                  1   \n",
       "9999               0         38190.78       0                  0   \n",
       "\n",
       "      Geography_Spain  Gender_Male  \n",
       "0                   0            0  \n",
       "1                   1            0  \n",
       "2                   0            0  \n",
       "3                   0            0  \n",
       "4                   1            0  \n",
       "...               ...          ...  \n",
       "9995                0            1  \n",
       "9996                0            1  \n",
       "9997                0            0  \n",
       "9998                0            1  \n",
       "9999                0            0  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d39e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['Exited'])\n",
    "y = df['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2957db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x_ss = ss.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "871787fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(x_ss,y,test_size=0.3 , random_state= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27506505",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = Sequential()\n",
    "s_model.add(Dense(10 , activation = 'ReLU' , kernel_regularizer = tensorflow.keras.regularizers.l2(0.003) , kernel_initializer='he_normal' , input_dim = 11))\n",
    "s_model.add(BatchNormalization())\n",
    "s_model.add(Dropout(0.2))\n",
    "s_model.add(Dense(5 , activation = 'ReLU' , kernel_regularizer = tensorflow.keras.regularizers.l2(0.003) , kernel_initializer='he_normal' ,input_dim = 10))\n",
    "s_model.add(BatchNormalization())\n",
    "s_model.add(Dropout(0.2))\n",
    "s_model.add(Dense(1 , activation = 'ReLU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f434de1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                120       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 10)                40        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 5)                 20        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 241 (964.00 Byte)\n",
      "Trainable params: 211 (844.00 Byte)\n",
      "Non-trainable params: 30 (120.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09e85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model.compile(loss = 'binary_crossentropy' , optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9943c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(\n",
    "    monitor = \"val_loss\",\n",
    "    min_delta = 0.00001,\n",
    "    patience = 20,\n",
    "    verbose = 1 ,\n",
    "    mode = \"auto\" ,\n",
    "    baseline = None ,\n",
    "    restore_best_weights = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1c57d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3500\n",
      "154/154 [==============================] - 1s 2ms/step - loss: 3.5677 - accuracy: 0.6514 - val_loss: 2.5859 - val_accuracy: 0.7067\n",
      "Epoch 2/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 3.3278 - accuracy: 0.6769 - val_loss: 2.5965 - val_accuracy: 0.7286\n",
      "Epoch 3/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 3.0602 - accuracy: 0.6988 - val_loss: 2.6158 - val_accuracy: 0.7476\n",
      "Epoch 4/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.9480 - accuracy: 0.7110 - val_loss: 2.4976 - val_accuracy: 0.7524\n",
      "Epoch 5/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.7764 - accuracy: 0.7314 - val_loss: 2.4880 - val_accuracy: 0.7614\n",
      "Epoch 6/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.7960 - accuracy: 0.7231 - val_loss: 2.4685 - val_accuracy: 0.7676\n",
      "Epoch 7/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.8121 - accuracy: 0.7249 - val_loss: 2.4103 - val_accuracy: 0.7686\n",
      "Epoch 8/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.7421 - accuracy: 0.7208 - val_loss: 2.2303 - val_accuracy: 0.7643\n",
      "Epoch 9/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.6960 - accuracy: 0.7257 - val_loss: 2.1757 - val_accuracy: 0.7671\n",
      "Epoch 10/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.5380 - accuracy: 0.7222 - val_loss: 2.0156 - val_accuracy: 0.7719\n",
      "Epoch 11/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.4815 - accuracy: 0.7251 - val_loss: 2.0101 - val_accuracy: 0.7738\n",
      "Epoch 12/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.3340 - accuracy: 0.7341 - val_loss: 1.9903 - val_accuracy: 0.7733\n",
      "Epoch 13/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.4500 - accuracy: 0.7286 - val_loss: 1.9552 - val_accuracy: 0.7733\n",
      "Epoch 14/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.2426 - accuracy: 0.7386 - val_loss: 1.8007 - val_accuracy: 0.7705\n",
      "Epoch 15/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.3027 - accuracy: 0.7320 - val_loss: 1.8107 - val_accuracy: 0.7786\n",
      "Epoch 16/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.2117 - accuracy: 0.7382 - val_loss: 1.7367 - val_accuracy: 0.7814\n",
      "Epoch 17/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.0983 - accuracy: 0.7422 - val_loss: 1.6566 - val_accuracy: 0.7829\n",
      "Epoch 18/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.1109 - accuracy: 0.7276 - val_loss: 1.5975 - val_accuracy: 0.7786\n",
      "Epoch 19/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 2.0427 - accuracy: 0.7322 - val_loss: 1.5586 - val_accuracy: 0.7838\n",
      "Epoch 20/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.9143 - accuracy: 0.7433 - val_loss: 1.4895 - val_accuracy: 0.7781\n",
      "Epoch 21/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.8568 - accuracy: 0.7467 - val_loss: 1.3766 - val_accuracy: 0.7810\n",
      "Epoch 22/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.8858 - accuracy: 0.7580 - val_loss: 1.4048 - val_accuracy: 0.7824\n",
      "Epoch 23/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.7928 - accuracy: 0.7643 - val_loss: 1.3903 - val_accuracy: 0.7819\n",
      "Epoch 24/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.8528 - accuracy: 0.7531 - val_loss: 1.3899 - val_accuracy: 0.7714\n",
      "Epoch 25/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.7641 - accuracy: 0.7508 - val_loss: 1.3688 - val_accuracy: 0.7743\n",
      "Epoch 26/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.7078 - accuracy: 0.7545 - val_loss: 1.3291 - val_accuracy: 0.7757\n",
      "Epoch 27/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.8039 - accuracy: 0.7512 - val_loss: 1.3077 - val_accuracy: 0.7752\n",
      "Epoch 28/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.6746 - accuracy: 0.7402 - val_loss: 1.3237 - val_accuracy: 0.7671\n",
      "Epoch 29/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.6624 - accuracy: 0.7404 - val_loss: 1.3267 - val_accuracy: 0.7686\n",
      "Epoch 30/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.5946 - accuracy: 0.7398 - val_loss: 1.3018 - val_accuracy: 0.7695\n",
      "Epoch 31/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.6058 - accuracy: 0.7359 - val_loss: 1.2901 - val_accuracy: 0.7724\n",
      "Epoch 32/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.6177 - accuracy: 0.7324 - val_loss: 1.2692 - val_accuracy: 0.7729\n",
      "Epoch 33/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.5465 - accuracy: 0.7371 - val_loss: 1.2957 - val_accuracy: 0.7724\n",
      "Epoch 34/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.6314 - accuracy: 0.7343 - val_loss: 1.2561 - val_accuracy: 0.7852\n",
      "Epoch 35/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.5988 - accuracy: 0.7773 - val_loss: 1.2925 - val_accuracy: 0.7862\n",
      "Epoch 36/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.4943 - accuracy: 0.7727 - val_loss: 1.2850 - val_accuracy: 0.7857\n",
      "Epoch 37/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.5217 - accuracy: 0.7702 - val_loss: 1.2844 - val_accuracy: 0.7867\n",
      "Epoch 38/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.5251 - accuracy: 0.7700 - val_loss: 1.2492 - val_accuracy: 0.7871\n",
      "Epoch 39/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.4513 - accuracy: 0.7724 - val_loss: 1.2362 - val_accuracy: 0.7867\n",
      "Epoch 40/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.4080 - accuracy: 0.7663 - val_loss: 1.2076 - val_accuracy: 0.7852\n",
      "Epoch 41/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.3853 - accuracy: 0.7635 - val_loss: 1.1845 - val_accuracy: 0.7862\n",
      "Epoch 42/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.2790 - accuracy: 0.7543 - val_loss: 1.1732 - val_accuracy: 0.7876\n",
      "Epoch 43/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 1.2856 - accuracy: 0.7665 - val_loss: 1.1872 - val_accuracy: 0.7881\n",
      "Epoch 44/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 1.2981 - accuracy: 0.7576 - val_loss: 1.1968 - val_accuracy: 0.7900\n",
      "Epoch 45/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 1.2837 - accuracy: 0.7616 - val_loss: 1.1548 - val_accuracy: 0.7895\n",
      "Epoch 46/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 1.2737 - accuracy: 0.7565 - val_loss: 1.1679 - val_accuracy: 0.7905\n",
      "Epoch 47/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 1.2301 - accuracy: 0.7724 - val_loss: 1.1560 - val_accuracy: 0.7905\n",
      "Epoch 48/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.2640 - accuracy: 0.7614 - val_loss: 1.1337 - val_accuracy: 0.7910\n",
      "Epoch 49/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.2486 - accuracy: 0.7647 - val_loss: 1.1450 - val_accuracy: 0.7910\n",
      "Epoch 50/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.2010 - accuracy: 0.7647 - val_loss: 1.1255 - val_accuracy: 0.7919\n",
      "Epoch 51/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.1878 - accuracy: 0.7682 - val_loss: 1.1044 - val_accuracy: 0.7895\n",
      "Epoch 52/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.1894 - accuracy: 0.7531 - val_loss: 1.0567 - val_accuracy: 0.7924\n",
      "Epoch 53/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.1276 - accuracy: 0.7620 - val_loss: 1.0412 - val_accuracy: 0.7919\n",
      "Epoch 54/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.1170 - accuracy: 0.7588 - val_loss: 1.0375 - val_accuracy: 0.7952\n",
      "Epoch 55/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.1687 - accuracy: 0.7641 - val_loss: 1.0415 - val_accuracy: 0.7952\n",
      "Epoch 56/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0744 - accuracy: 0.7671 - val_loss: 1.0225 - val_accuracy: 0.7971\n",
      "Epoch 57/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0726 - accuracy: 0.7604 - val_loss: 1.0269 - val_accuracy: 0.7952\n",
      "Epoch 58/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0609 - accuracy: 0.7710 - val_loss: 1.0038 - val_accuracy: 0.7952\n",
      "Epoch 59/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0473 - accuracy: 0.7727 - val_loss: 1.0053 - val_accuracy: 0.7938\n",
      "Epoch 60/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0689 - accuracy: 0.7692 - val_loss: 0.9999 - val_accuracy: 0.7938\n",
      "Epoch 61/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0480 - accuracy: 0.7602 - val_loss: 0.9334 - val_accuracy: 0.7862\n",
      "Epoch 62/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0118 - accuracy: 0.7482 - val_loss: 0.9260 - val_accuracy: 0.7843\n",
      "Epoch 63/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.9838 - accuracy: 0.7447 - val_loss: 0.9277 - val_accuracy: 0.7871\n",
      "Epoch 64/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0229 - accuracy: 0.7469 - val_loss: 0.9284 - val_accuracy: 0.7914\n",
      "Epoch 65/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0022 - accuracy: 0.7473 - val_loss: 0.9132 - val_accuracy: 0.7905\n",
      "Epoch 66/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0125 - accuracy: 0.7504 - val_loss: 0.9251 - val_accuracy: 0.7919\n",
      "Epoch 67/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.9747 - accuracy: 0.7657 - val_loss: 0.9295 - val_accuracy: 0.7910\n",
      "Epoch 68/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 1.0008 - accuracy: 0.7743 - val_loss: 0.9346 - val_accuracy: 0.7895\n",
      "Epoch 69/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.9201 - accuracy: 0.7657 - val_loss: 0.9279 - val_accuracy: 0.7895\n",
      "Epoch 70/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.9726 - accuracy: 0.7665 - val_loss: 0.9095 - val_accuracy: 0.7886\n",
      "Epoch 71/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8953 - accuracy: 0.7704 - val_loss: 0.8902 - val_accuracy: 0.7890\n",
      "Epoch 72/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.9328 - accuracy: 0.7739 - val_loss: 0.9094 - val_accuracy: 0.7852\n",
      "Epoch 73/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.9336 - accuracy: 0.7720 - val_loss: 0.8915 - val_accuracy: 0.7843\n",
      "Epoch 74/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8980 - accuracy: 0.7767 - val_loss: 0.8834 - val_accuracy: 0.7857\n",
      "Epoch 75/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.9067 - accuracy: 0.7761 - val_loss: 0.8756 - val_accuracy: 0.7881\n",
      "Epoch 76/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.9200 - accuracy: 0.7776 - val_loss: 0.8629 - val_accuracy: 0.7895\n",
      "Epoch 77/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8489 - accuracy: 0.7867 - val_loss: 0.8447 - val_accuracy: 0.7905\n",
      "Epoch 78/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8381 - accuracy: 0.7845 - val_loss: 0.8440 - val_accuracy: 0.7895\n",
      "Epoch 79/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8778 - accuracy: 0.7831 - val_loss: 0.8383 - val_accuracy: 0.7895\n",
      "Epoch 80/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8157 - accuracy: 0.7886 - val_loss: 0.8332 - val_accuracy: 0.7914\n",
      "Epoch 81/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8431 - accuracy: 0.7859 - val_loss: 0.8279 - val_accuracy: 0.7914\n",
      "Epoch 82/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8383 - accuracy: 0.7873 - val_loss: 0.8273 - val_accuracy: 0.7919\n",
      "Epoch 83/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8371 - accuracy: 0.7843 - val_loss: 0.8328 - val_accuracy: 0.7914\n",
      "Epoch 84/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8464 - accuracy: 0.7871 - val_loss: 0.8201 - val_accuracy: 0.7905\n",
      "Epoch 85/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8422 - accuracy: 0.7906 - val_loss: 0.8160 - val_accuracy: 0.7914\n",
      "Epoch 86/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8411 - accuracy: 0.7863 - val_loss: 0.8155 - val_accuracy: 0.7890\n",
      "Epoch 87/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8412 - accuracy: 0.7869 - val_loss: 0.8070 - val_accuracy: 0.7890\n",
      "Epoch 88/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8112 - accuracy: 0.7896 - val_loss: 0.7817 - val_accuracy: 0.7895\n",
      "Epoch 89/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7984 - accuracy: 0.7916 - val_loss: 0.7750 - val_accuracy: 0.7886\n",
      "Epoch 90/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8060 - accuracy: 0.7927 - val_loss: 0.7709 - val_accuracy: 0.7886\n",
      "Epoch 91/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.8113 - accuracy: 0.7935 - val_loss: 0.7733 - val_accuracy: 0.7886\n",
      "Epoch 92/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7983 - accuracy: 0.7990 - val_loss: 0.7667 - val_accuracy: 0.7886\n",
      "Epoch 93/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7851 - accuracy: 0.7929 - val_loss: 0.7488 - val_accuracy: 0.7886\n",
      "Epoch 94/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7745 - accuracy: 0.7976 - val_loss: 0.7299 - val_accuracy: 0.7895\n",
      "Epoch 95/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7556 - accuracy: 0.7963 - val_loss: 0.7036 - val_accuracy: 0.7919\n",
      "Epoch 96/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7669 - accuracy: 0.7935 - val_loss: 0.6688 - val_accuracy: 0.7929\n",
      "Epoch 97/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7085 - accuracy: 0.7933 - val_loss: 0.6608 - val_accuracy: 0.7933\n",
      "Epoch 98/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7352 - accuracy: 0.7978 - val_loss: 0.6474 - val_accuracy: 0.7929\n",
      "Epoch 99/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7020 - accuracy: 0.7924 - val_loss: 0.6401 - val_accuracy: 0.7929\n",
      "Epoch 100/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7000 - accuracy: 0.7947 - val_loss: 0.6373 - val_accuracy: 0.7919\n",
      "Epoch 101/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7287 - accuracy: 0.7984 - val_loss: 0.6366 - val_accuracy: 0.7929\n",
      "Epoch 102/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.7236 - accuracy: 0.7980 - val_loss: 0.6270 - val_accuracy: 0.7919\n",
      "Epoch 103/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6679 - accuracy: 0.8020 - val_loss: 0.6182 - val_accuracy: 0.7938\n",
      "Epoch 104/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6800 - accuracy: 0.7937 - val_loss: 0.6050 - val_accuracy: 0.7924\n",
      "Epoch 105/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6530 - accuracy: 0.8006 - val_loss: 0.6092 - val_accuracy: 0.7929\n",
      "Epoch 106/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6647 - accuracy: 0.8014 - val_loss: 0.6077 - val_accuracy: 0.7919\n",
      "Epoch 107/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6383 - accuracy: 0.7982 - val_loss: 0.5999 - val_accuracy: 0.7914\n",
      "Epoch 108/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6639 - accuracy: 0.7990 - val_loss: 0.6034 - val_accuracy: 0.7914\n",
      "Epoch 109/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6208 - accuracy: 0.8027 - val_loss: 0.5884 - val_accuracy: 0.7914\n",
      "Epoch 110/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6100 - accuracy: 0.8008 - val_loss: 0.5821 - val_accuracy: 0.7938\n",
      "Epoch 111/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6096 - accuracy: 0.8002 - val_loss: 0.5805 - val_accuracy: 0.7919\n",
      "Epoch 112/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5943 - accuracy: 0.7984 - val_loss: 0.5790 - val_accuracy: 0.7919\n",
      "Epoch 113/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6068 - accuracy: 0.7978 - val_loss: 0.5755 - val_accuracy: 0.7938\n",
      "Epoch 114/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5891 - accuracy: 0.8022 - val_loss: 0.5780 - val_accuracy: 0.7914\n",
      "Epoch 115/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.8002 - val_loss: 0.5746 - val_accuracy: 0.7933\n",
      "Epoch 116/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.8035 - val_loss: 0.5773 - val_accuracy: 0.7914\n",
      "Epoch 117/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.6145 - accuracy: 0.8045 - val_loss: 0.5559 - val_accuracy: 0.7919\n",
      "Epoch 118/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5951 - accuracy: 0.8037 - val_loss: 0.5648 - val_accuracy: 0.7929\n",
      "Epoch 119/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5772 - accuracy: 0.8073 - val_loss: 0.5600 - val_accuracy: 0.7952\n",
      "Epoch 120/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5885 - accuracy: 0.8022 - val_loss: 0.5622 - val_accuracy: 0.7948\n",
      "Epoch 121/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5579 - accuracy: 0.8010 - val_loss: 0.5568 - val_accuracy: 0.7952\n",
      "Epoch 122/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5854 - accuracy: 0.8022 - val_loss: 0.5528 - val_accuracy: 0.7938\n",
      "Epoch 123/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5770 - accuracy: 0.8035 - val_loss: 0.5491 - val_accuracy: 0.7943\n",
      "Epoch 124/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5815 - accuracy: 0.8029 - val_loss: 0.5451 - val_accuracy: 0.7938\n",
      "Epoch 125/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5664 - accuracy: 0.8008 - val_loss: 0.5431 - val_accuracy: 0.7933\n",
      "Epoch 126/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.8076 - val_loss: 0.5473 - val_accuracy: 0.7943\n",
      "Epoch 127/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.8049 - val_loss: 0.5384 - val_accuracy: 0.7967\n",
      "Epoch 128/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5712 - accuracy: 0.8045 - val_loss: 0.5371 - val_accuracy: 0.7948\n",
      "Epoch 129/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.8108 - val_loss: 0.5345 - val_accuracy: 0.7962\n",
      "Epoch 130/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5749 - accuracy: 0.8055 - val_loss: 0.5318 - val_accuracy: 0.7971\n",
      "Epoch 131/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.8094 - val_loss: 0.5313 - val_accuracy: 0.7981\n",
      "Epoch 132/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.8012 - val_loss: 0.5303 - val_accuracy: 0.7986\n",
      "Epoch 133/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5461 - accuracy: 0.8078 - val_loss: 0.5302 - val_accuracy: 0.7990\n",
      "Epoch 134/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.8094 - val_loss: 0.5413 - val_accuracy: 0.7976\n",
      "Epoch 135/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5644 - accuracy: 0.8041 - val_loss: 0.5270 - val_accuracy: 0.7995\n",
      "Epoch 136/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5494 - accuracy: 0.8073 - val_loss: 0.5261 - val_accuracy: 0.8000\n",
      "Epoch 137/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.8020 - val_loss: 0.5225 - val_accuracy: 0.8010\n",
      "Epoch 138/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5312 - accuracy: 0.8084 - val_loss: 0.5226 - val_accuracy: 0.8014\n",
      "Epoch 139/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5231 - accuracy: 0.8039 - val_loss: 0.5283 - val_accuracy: 0.7943\n",
      "Epoch 140/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.8045 - val_loss: 0.5219 - val_accuracy: 0.7971\n",
      "Epoch 141/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5316 - accuracy: 0.8080 - val_loss: 0.5074 - val_accuracy: 0.7990\n",
      "Epoch 142/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.8071 - val_loss: 0.5092 - val_accuracy: 0.7995\n",
      "Epoch 143/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5203 - accuracy: 0.8084 - val_loss: 0.5025 - val_accuracy: 0.7957\n",
      "Epoch 144/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.8082 - val_loss: 0.4997 - val_accuracy: 0.7971\n",
      "Epoch 145/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5319 - accuracy: 0.8043 - val_loss: 0.5026 - val_accuracy: 0.7957\n",
      "Epoch 146/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.8090 - val_loss: 0.4952 - val_accuracy: 0.7995\n",
      "Epoch 147/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5274 - accuracy: 0.8073 - val_loss: 0.5200 - val_accuracy: 0.7952\n",
      "Epoch 148/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5481 - accuracy: 0.8090 - val_loss: 0.5166 - val_accuracy: 0.7967\n",
      "Epoch 149/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.8071 - val_loss: 0.5125 - val_accuracy: 0.7971\n",
      "Epoch 150/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5314 - accuracy: 0.8090 - val_loss: 0.5076 - val_accuracy: 0.7971\n",
      "Epoch 151/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5287 - accuracy: 0.8067 - val_loss: 0.5031 - val_accuracy: 0.7981\n",
      "Epoch 152/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5024 - accuracy: 0.8047 - val_loss: 0.4969 - val_accuracy: 0.7976\n",
      "Epoch 153/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.8057 - val_loss: 0.4951 - val_accuracy: 0.7976\n",
      "Epoch 154/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5284 - accuracy: 0.8078 - val_loss: 0.4927 - val_accuracy: 0.7990\n",
      "Epoch 155/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7996 - val_loss: 0.4904 - val_accuracy: 0.8000\n",
      "Epoch 156/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5111 - accuracy: 0.8031 - val_loss: 0.4890 - val_accuracy: 0.8000\n",
      "Epoch 157/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5069 - accuracy: 0.8114 - val_loss: 0.4935 - val_accuracy: 0.8005\n",
      "Epoch 158/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.8082 - val_loss: 0.4898 - val_accuracy: 0.8019\n",
      "Epoch 159/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5083 - accuracy: 0.8149 - val_loss: 0.4889 - val_accuracy: 0.7990\n",
      "Epoch 160/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4952 - accuracy: 0.8061 - val_loss: 0.5067 - val_accuracy: 0.7881\n",
      "Epoch 161/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7971 - val_loss: 0.4999 - val_accuracy: 0.7876\n",
      "Epoch 162/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.7973 - val_loss: 0.4943 - val_accuracy: 0.7943\n",
      "Epoch 163/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5086 - accuracy: 0.8037 - val_loss: 0.4947 - val_accuracy: 0.7952\n",
      "Epoch 164/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4983 - accuracy: 0.8051 - val_loss: 0.4853 - val_accuracy: 0.7957\n",
      "Epoch 165/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4968 - accuracy: 0.8100 - val_loss: 0.4829 - val_accuracy: 0.7981\n",
      "Epoch 166/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5113 - accuracy: 0.8059 - val_loss: 0.4817 - val_accuracy: 0.7976\n",
      "Epoch 167/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5149 - accuracy: 0.8065 - val_loss: 0.4792 - val_accuracy: 0.8014\n",
      "Epoch 168/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.8059 - val_loss: 0.4769 - val_accuracy: 0.8019\n",
      "Epoch 169/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.8084 - val_loss: 0.4710 - val_accuracy: 0.8005\n",
      "Epoch 170/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4936 - accuracy: 0.8061 - val_loss: 0.4699 - val_accuracy: 0.8000\n",
      "Epoch 171/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4942 - accuracy: 0.8108 - val_loss: 0.4685 - val_accuracy: 0.8014\n",
      "Epoch 172/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4848 - accuracy: 0.8094 - val_loss: 0.4604 - val_accuracy: 0.8019\n",
      "Epoch 173/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4975 - accuracy: 0.8114 - val_loss: 0.4661 - val_accuracy: 0.8014\n",
      "Epoch 174/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5036 - accuracy: 0.8086 - val_loss: 0.4658 - val_accuracy: 0.8024\n",
      "Epoch 175/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.8122 - val_loss: 0.4660 - val_accuracy: 0.8048\n",
      "Epoch 176/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4912 - accuracy: 0.8096 - val_loss: 0.4509 - val_accuracy: 0.8043\n",
      "Epoch 177/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.8086 - val_loss: 0.4509 - val_accuracy: 0.8043\n",
      "Epoch 178/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.8135 - val_loss: 0.4477 - val_accuracy: 0.8062\n",
      "Epoch 179/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4917 - accuracy: 0.8078 - val_loss: 0.4466 - val_accuracy: 0.8052\n",
      "Epoch 180/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4800 - accuracy: 0.8131 - val_loss: 0.4491 - val_accuracy: 0.8043\n",
      "Epoch 181/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.8124 - val_loss: 0.4504 - val_accuracy: 0.8062\n",
      "Epoch 182/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4968 - accuracy: 0.8114 - val_loss: 0.4535 - val_accuracy: 0.8124\n",
      "Epoch 183/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4820 - accuracy: 0.8131 - val_loss: 0.4452 - val_accuracy: 0.8143\n",
      "Epoch 184/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.8127 - val_loss: 0.4439 - val_accuracy: 0.8105\n",
      "Epoch 185/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.8078 - val_loss: 0.4657 - val_accuracy: 0.8014\n",
      "Epoch 186/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4857 - accuracy: 0.8078 - val_loss: 0.4598 - val_accuracy: 0.8052\n",
      "Epoch 187/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.8076 - val_loss: 0.4544 - val_accuracy: 0.8043\n",
      "Epoch 188/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.8071 - val_loss: 0.4504 - val_accuracy: 0.8052\n",
      "Epoch 189/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4879 - accuracy: 0.8080 - val_loss: 0.4499 - val_accuracy: 0.8067\n",
      "Epoch 190/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.8137 - val_loss: 0.4459 - val_accuracy: 0.8100\n",
      "Epoch 191/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.8131 - val_loss: 0.4448 - val_accuracy: 0.8105\n",
      "Epoch 192/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4688 - accuracy: 0.8161 - val_loss: 0.4456 - val_accuracy: 0.8095\n",
      "Epoch 193/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.8151 - val_loss: 0.4453 - val_accuracy: 0.8110\n",
      "Epoch 194/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.8141 - val_loss: 0.4424 - val_accuracy: 0.8138\n",
      "Epoch 195/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.8127 - val_loss: 0.4445 - val_accuracy: 0.8090\n",
      "Epoch 196/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.8120 - val_loss: 0.4612 - val_accuracy: 0.7967\n",
      "Epoch 197/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4624 - accuracy: 0.8082 - val_loss: 0.4508 - val_accuracy: 0.8052\n",
      "Epoch 198/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4636 - accuracy: 0.8118 - val_loss: 0.4469 - val_accuracy: 0.8067\n",
      "Epoch 199/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4636 - accuracy: 0.8116 - val_loss: 0.4434 - val_accuracy: 0.8105\n",
      "Epoch 200/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4661 - accuracy: 0.8116 - val_loss: 0.4419 - val_accuracy: 0.8105\n",
      "Epoch 201/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4597 - accuracy: 0.8171 - val_loss: 0.4386 - val_accuracy: 0.8110\n",
      "Epoch 202/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.8157 - val_loss: 0.4542 - val_accuracy: 0.8157\n",
      "Epoch 203/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4632 - accuracy: 0.8139 - val_loss: 0.4520 - val_accuracy: 0.8090\n",
      "Epoch 204/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4671 - accuracy: 0.8133 - val_loss: 0.4456 - val_accuracy: 0.8181\n",
      "Epoch 205/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.8104 - val_loss: 0.4413 - val_accuracy: 0.8181\n",
      "Epoch 206/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.8165 - val_loss: 0.4386 - val_accuracy: 0.8186\n",
      "Epoch 207/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.8180 - val_loss: 0.4373 - val_accuracy: 0.8171\n",
      "Epoch 208/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.8176 - val_loss: 0.4383 - val_accuracy: 0.8119\n",
      "Epoch 209/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4716 - accuracy: 0.8145 - val_loss: 0.4373 - val_accuracy: 0.8148\n",
      "Epoch 210/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4619 - accuracy: 0.8153 - val_loss: 0.4352 - val_accuracy: 0.8162\n",
      "Epoch 211/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4521 - accuracy: 0.8161 - val_loss: 0.4389 - val_accuracy: 0.8110\n",
      "Epoch 212/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4519 - accuracy: 0.8137 - val_loss: 0.4370 - val_accuracy: 0.8138\n",
      "Epoch 213/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4670 - accuracy: 0.8124 - val_loss: 0.4628 - val_accuracy: 0.7976\n",
      "Epoch 214/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.8100 - val_loss: 0.4546 - val_accuracy: 0.8010\n",
      "Epoch 215/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.8100 - val_loss: 0.4529 - val_accuracy: 0.8086\n",
      "Epoch 216/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.8129 - val_loss: 0.4477 - val_accuracy: 0.8086\n",
      "Epoch 217/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.8173 - val_loss: 0.4454 - val_accuracy: 0.8105\n",
      "Epoch 218/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.8149 - val_loss: 0.4437 - val_accuracy: 0.8110\n",
      "Epoch 219/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4536 - accuracy: 0.8186 - val_loss: 0.4411 - val_accuracy: 0.8133\n",
      "Epoch 220/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4632 - accuracy: 0.8137 - val_loss: 0.4414 - val_accuracy: 0.8129\n",
      "Epoch 221/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8133 - val_loss: 0.4450 - val_accuracy: 0.8100\n",
      "Epoch 222/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4522 - accuracy: 0.8122 - val_loss: 0.4406 - val_accuracy: 0.8148\n",
      "Epoch 223/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4473 - accuracy: 0.8131 - val_loss: 0.4380 - val_accuracy: 0.8148\n",
      "Epoch 224/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8186 - val_loss: 0.4373 - val_accuracy: 0.8157\n",
      "Epoch 225/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4437 - accuracy: 0.8139 - val_loss: 0.4439 - val_accuracy: 0.8052\n",
      "Epoch 226/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4448 - accuracy: 0.8151 - val_loss: 0.4379 - val_accuracy: 0.8114\n",
      "Epoch 227/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4419 - accuracy: 0.8155 - val_loss: 0.4428 - val_accuracy: 0.8062\n",
      "Epoch 228/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8194 - val_loss: 0.4322 - val_accuracy: 0.8152\n",
      "Epoch 229/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.8131 - val_loss: 0.4267 - val_accuracy: 0.8190\n",
      "Epoch 230/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.8165 - val_loss: 0.4295 - val_accuracy: 0.8176\n",
      "Epoch 231/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.8198 - val_loss: 0.4300 - val_accuracy: 0.8181\n",
      "Epoch 232/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4454 - accuracy: 0.8145 - val_loss: 0.4266 - val_accuracy: 0.8195\n",
      "Epoch 233/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4428 - accuracy: 0.8141 - val_loss: 0.4263 - val_accuracy: 0.8176\n",
      "Epoch 234/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8173 - val_loss: 0.4220 - val_accuracy: 0.8210\n",
      "Epoch 235/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.8233 - val_loss: 0.4211 - val_accuracy: 0.8248\n",
      "Epoch 236/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8180 - val_loss: 0.4218 - val_accuracy: 0.8252\n",
      "Epoch 237/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.8233 - val_loss: 0.4199 - val_accuracy: 0.8281\n",
      "Epoch 238/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.8196 - val_loss: 0.4183 - val_accuracy: 0.8257\n",
      "Epoch 239/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.8173 - val_loss: 0.4276 - val_accuracy: 0.8276\n",
      "Epoch 240/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8196 - val_loss: 0.4230 - val_accuracy: 0.8248\n",
      "Epoch 241/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4386 - accuracy: 0.8200 - val_loss: 0.4191 - val_accuracy: 0.8257\n",
      "Epoch 242/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.8194 - val_loss: 0.4201 - val_accuracy: 0.8233\n",
      "Epoch 243/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4361 - accuracy: 0.8129 - val_loss: 0.4206 - val_accuracy: 0.8267\n",
      "Epoch 244/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8210 - val_loss: 0.4299 - val_accuracy: 0.8119\n",
      "Epoch 245/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4336 - accuracy: 0.8098 - val_loss: 0.4358 - val_accuracy: 0.8100\n",
      "Epoch 246/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8165 - val_loss: 0.4204 - val_accuracy: 0.8229\n",
      "Epoch 247/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4211 - accuracy: 0.8190 - val_loss: 0.4167 - val_accuracy: 0.8229\n",
      "Epoch 248/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8149 - val_loss: 0.4223 - val_accuracy: 0.8210\n",
      "Epoch 249/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.8269 - val_loss: 0.4169 - val_accuracy: 0.8252\n",
      "Epoch 250/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.8184 - val_loss: 0.4247 - val_accuracy: 0.8195\n",
      "Epoch 251/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8245 - val_loss: 0.4155 - val_accuracy: 0.8243\n",
      "Epoch 252/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8235 - val_loss: 0.4144 - val_accuracy: 0.8290\n",
      "Epoch 253/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4279 - accuracy: 0.8241 - val_loss: 0.4105 - val_accuracy: 0.8324\n",
      "Epoch 254/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8212 - val_loss: 0.4096 - val_accuracy: 0.8314\n",
      "Epoch 255/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4288 - accuracy: 0.8265 - val_loss: 0.4073 - val_accuracy: 0.8329\n",
      "Epoch 256/3500\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8192 - val_loss: 0.4152 - val_accuracy: 0.8295\n",
      "Epoch 257/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4121 - accuracy: 0.8224 - val_loss: 0.4120 - val_accuracy: 0.8333\n",
      "Epoch 258/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8257 - val_loss: 0.4075 - val_accuracy: 0.8319\n",
      "Epoch 259/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8255 - val_loss: 0.4198 - val_accuracy: 0.8252\n",
      "Epoch 260/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4193 - accuracy: 0.8247 - val_loss: 0.4137 - val_accuracy: 0.8286\n",
      "Epoch 261/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.8290 - val_loss: 0.4076 - val_accuracy: 0.8305\n",
      "Epoch 262/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4161 - accuracy: 0.8255 - val_loss: 0.4123 - val_accuracy: 0.8329\n",
      "Epoch 263/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4083 - accuracy: 0.8241 - val_loss: 0.4058 - val_accuracy: 0.8305\n",
      "Epoch 264/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4225 - accuracy: 0.8249 - val_loss: 0.4071 - val_accuracy: 0.8290\n",
      "Epoch 265/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4088 - accuracy: 0.8306 - val_loss: 0.4047 - val_accuracy: 0.8314\n",
      "Epoch 266/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.8220 - val_loss: 0.3983 - val_accuracy: 0.8343\n",
      "Epoch 267/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4138 - accuracy: 0.8280 - val_loss: 0.4036 - val_accuracy: 0.8314\n",
      "Epoch 268/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8314 - val_loss: 0.4000 - val_accuracy: 0.8390\n",
      "Epoch 269/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4160 - accuracy: 0.8300 - val_loss: 0.4054 - val_accuracy: 0.8262\n",
      "Epoch 270/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8241 - val_loss: 0.3964 - val_accuracy: 0.8343\n",
      "Epoch 271/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4182 - accuracy: 0.8286 - val_loss: 0.3949 - val_accuracy: 0.8362\n",
      "Epoch 272/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.8224 - val_loss: 0.4145 - val_accuracy: 0.8176\n",
      "Epoch 273/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4069 - accuracy: 0.8210 - val_loss: 0.4001 - val_accuracy: 0.8300\n",
      "Epoch 274/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8239 - val_loss: 0.4038 - val_accuracy: 0.8324\n",
      "Epoch 275/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4106 - accuracy: 0.8276 - val_loss: 0.3970 - val_accuracy: 0.8357\n",
      "Epoch 276/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8327 - val_loss: 0.3945 - val_accuracy: 0.8362\n",
      "Epoch 277/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4162 - accuracy: 0.8294 - val_loss: 0.3925 - val_accuracy: 0.8376\n",
      "Epoch 278/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3986 - accuracy: 0.8343 - val_loss: 0.3947 - val_accuracy: 0.8371\n",
      "Epoch 279/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4150 - accuracy: 0.8273 - val_loss: 0.4007 - val_accuracy: 0.8376\n",
      "Epoch 280/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3989 - accuracy: 0.8361 - val_loss: 0.3975 - val_accuracy: 0.8362\n",
      "Epoch 281/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4139 - accuracy: 0.8251 - val_loss: 0.4027 - val_accuracy: 0.8343\n",
      "Epoch 282/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4038 - accuracy: 0.8322 - val_loss: 0.3947 - val_accuracy: 0.8390\n",
      "Epoch 283/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8294 - val_loss: 0.3919 - val_accuracy: 0.8381\n",
      "Epoch 284/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4157 - accuracy: 0.8337 - val_loss: 0.4026 - val_accuracy: 0.8343\n",
      "Epoch 285/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4087 - accuracy: 0.8310 - val_loss: 0.4036 - val_accuracy: 0.8310\n",
      "Epoch 286/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8257 - val_loss: 0.3948 - val_accuracy: 0.8371\n",
      "Epoch 287/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8308 - val_loss: 0.4028 - val_accuracy: 0.8324\n",
      "Epoch 288/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4139 - accuracy: 0.8259 - val_loss: 0.4133 - val_accuracy: 0.8243\n",
      "Epoch 289/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4124 - accuracy: 0.8288 - val_loss: 0.4007 - val_accuracy: 0.8329\n",
      "Epoch 290/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8294 - val_loss: 0.3920 - val_accuracy: 0.8371\n",
      "Epoch 291/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4032 - accuracy: 0.8363 - val_loss: 0.4059 - val_accuracy: 0.8357\n",
      "Epoch 292/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4163 - accuracy: 0.8296 - val_loss: 0.4382 - val_accuracy: 0.8052\n",
      "Epoch 293/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8167 - val_loss: 0.4275 - val_accuracy: 0.8257\n",
      "Epoch 294/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4220 - accuracy: 0.8261 - val_loss: 0.4252 - val_accuracy: 0.8300\n",
      "Epoch 295/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4184 - accuracy: 0.8261 - val_loss: 0.4156 - val_accuracy: 0.8295\n",
      "Epoch 296/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4104 - accuracy: 0.8265 - val_loss: 0.4104 - val_accuracy: 0.8324\n",
      "Epoch 297/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4103 - accuracy: 0.8265 - val_loss: 0.4068 - val_accuracy: 0.8352\n",
      "Epoch 298/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8284 - val_loss: 0.4030 - val_accuracy: 0.8343\n",
      "Epoch 299/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4069 - accuracy: 0.8310 - val_loss: 0.3997 - val_accuracy: 0.8333\n",
      "Epoch 300/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4075 - accuracy: 0.8243 - val_loss: 0.3971 - val_accuracy: 0.8367\n",
      "Epoch 301/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4123 - accuracy: 0.8349 - val_loss: 0.3995 - val_accuracy: 0.8376\n",
      "Epoch 302/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4002 - accuracy: 0.8343 - val_loss: 0.3976 - val_accuracy: 0.8386\n",
      "Epoch 303/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.8286 - val_loss: 0.3905 - val_accuracy: 0.8390\n",
      "Epoch 304/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.8390 - val_loss: 0.3900 - val_accuracy: 0.8376\n",
      "Epoch 305/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8390 - val_loss: 0.3870 - val_accuracy: 0.8395\n",
      "Epoch 306/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.8371 - val_loss: 0.3909 - val_accuracy: 0.8343\n",
      "Epoch 307/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4104 - accuracy: 0.8265 - val_loss: 0.4003 - val_accuracy: 0.8333\n",
      "Epoch 308/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8324 - val_loss: 0.4012 - val_accuracy: 0.8357\n",
      "Epoch 309/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4007 - accuracy: 0.8300 - val_loss: 0.3967 - val_accuracy: 0.8371\n",
      "Epoch 310/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8292 - val_loss: 0.3884 - val_accuracy: 0.8390\n",
      "Epoch 311/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4007 - accuracy: 0.8308 - val_loss: 0.3900 - val_accuracy: 0.8333\n",
      "Epoch 312/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8308 - val_loss: 0.3885 - val_accuracy: 0.8333\n",
      "Epoch 313/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4215 - accuracy: 0.8286 - val_loss: 0.4145 - val_accuracy: 0.8300\n",
      "Epoch 314/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4108 - accuracy: 0.8324 - val_loss: 0.4033 - val_accuracy: 0.8376\n",
      "Epoch 315/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8331 - val_loss: 0.3983 - val_accuracy: 0.8410\n",
      "Epoch 316/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4096 - accuracy: 0.8347 - val_loss: 0.3929 - val_accuracy: 0.8410\n",
      "Epoch 317/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4089 - accuracy: 0.8280 - val_loss: 0.3931 - val_accuracy: 0.8371\n",
      "Epoch 318/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4134 - accuracy: 0.8296 - val_loss: 0.3901 - val_accuracy: 0.8410\n",
      "Epoch 319/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8384 - val_loss: 0.3898 - val_accuracy: 0.8376\n",
      "Epoch 320/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3964 - accuracy: 0.8290 - val_loss: 0.3890 - val_accuracy: 0.8362\n",
      "Epoch 321/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3869 - accuracy: 0.8357 - val_loss: 0.3857 - val_accuracy: 0.8381\n",
      "Epoch 322/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8378 - val_loss: 0.3866 - val_accuracy: 0.8371\n",
      "Epoch 323/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4035 - accuracy: 0.8310 - val_loss: 0.3963 - val_accuracy: 0.8386\n",
      "Epoch 324/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3858 - accuracy: 0.8308 - val_loss: 0.3935 - val_accuracy: 0.8400\n",
      "Epoch 325/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3904 - accuracy: 0.8343 - val_loss: 0.3892 - val_accuracy: 0.8395\n",
      "Epoch 326/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4146 - accuracy: 0.8314 - val_loss: 0.3915 - val_accuracy: 0.8395\n",
      "Epoch 327/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4017 - accuracy: 0.8341 - val_loss: 0.3992 - val_accuracy: 0.8386\n",
      "Epoch 328/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8312 - val_loss: 0.4062 - val_accuracy: 0.8305\n",
      "Epoch 329/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3990 - accuracy: 0.8284 - val_loss: 0.3936 - val_accuracy: 0.8395\n",
      "Epoch 330/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8337 - val_loss: 0.3875 - val_accuracy: 0.8414\n",
      "Epoch 331/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4075 - accuracy: 0.8314 - val_loss: 0.3931 - val_accuracy: 0.8395\n",
      "Epoch 332/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8282 - val_loss: 0.3843 - val_accuracy: 0.8429\n",
      "Epoch 333/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3924 - accuracy: 0.8300 - val_loss: 0.3952 - val_accuracy: 0.8410\n",
      "Epoch 334/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8310 - val_loss: 0.3961 - val_accuracy: 0.8395\n",
      "Epoch 335/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8312 - val_loss: 0.3850 - val_accuracy: 0.8400\n",
      "Epoch 336/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.8304 - val_loss: 0.3941 - val_accuracy: 0.8410\n",
      "Epoch 337/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.8257 - val_loss: 0.3955 - val_accuracy: 0.8262\n",
      "Epoch 338/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3950 - accuracy: 0.8296 - val_loss: 0.3940 - val_accuracy: 0.8281\n",
      "Epoch 339/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8300 - val_loss: 0.3883 - val_accuracy: 0.8376\n",
      "Epoch 340/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3967 - accuracy: 0.8337 - val_loss: 0.3877 - val_accuracy: 0.8381\n",
      "Epoch 341/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3903 - accuracy: 0.8320 - val_loss: 0.3862 - val_accuracy: 0.8405\n",
      "Epoch 342/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4002 - accuracy: 0.8263 - val_loss: 0.3963 - val_accuracy: 0.8343\n",
      "Epoch 343/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3866 - accuracy: 0.8329 - val_loss: 0.3897 - val_accuracy: 0.8395\n",
      "Epoch 344/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8329 - val_loss: 0.3924 - val_accuracy: 0.8381\n",
      "Epoch 345/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.8316 - val_loss: 0.3898 - val_accuracy: 0.8429\n",
      "Epoch 346/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4093 - accuracy: 0.8331 - val_loss: 0.3962 - val_accuracy: 0.8367\n",
      "Epoch 347/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3911 - accuracy: 0.8369 - val_loss: 0.3839 - val_accuracy: 0.8433\n",
      "Epoch 348/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4055 - accuracy: 0.8271 - val_loss: 0.3890 - val_accuracy: 0.8371\n",
      "Epoch 349/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4026 - accuracy: 0.8292 - val_loss: 0.3910 - val_accuracy: 0.8362\n",
      "Epoch 350/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3887 - accuracy: 0.8327 - val_loss: 0.3858 - val_accuracy: 0.8367\n",
      "Epoch 351/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3957 - accuracy: 0.8267 - val_loss: 0.4085 - val_accuracy: 0.8186\n",
      "Epoch 352/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8229 - val_loss: 0.3914 - val_accuracy: 0.8357\n",
      "Epoch 353/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8296 - val_loss: 0.3842 - val_accuracy: 0.8405\n",
      "Epoch 354/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3919 - accuracy: 0.8357 - val_loss: 0.3852 - val_accuracy: 0.8405\n",
      "Epoch 355/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3898 - accuracy: 0.8333 - val_loss: 0.3828 - val_accuracy: 0.8410\n",
      "Epoch 356/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8324 - val_loss: 0.3872 - val_accuracy: 0.8410\n",
      "Epoch 357/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3856 - accuracy: 0.8351 - val_loss: 0.3843 - val_accuracy: 0.8405\n",
      "Epoch 358/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8304 - val_loss: 0.3826 - val_accuracy: 0.8390\n",
      "Epoch 359/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8376 - val_loss: 0.3810 - val_accuracy: 0.8405\n",
      "Epoch 360/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4035 - accuracy: 0.8347 - val_loss: 0.3955 - val_accuracy: 0.8343\n",
      "Epoch 361/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4082 - accuracy: 0.8300 - val_loss: 0.3878 - val_accuracy: 0.8371\n",
      "Epoch 362/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3924 - accuracy: 0.8316 - val_loss: 0.3900 - val_accuracy: 0.8376\n",
      "Epoch 363/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3996 - accuracy: 0.8284 - val_loss: 0.3948 - val_accuracy: 0.8352\n",
      "Epoch 364/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3860 - accuracy: 0.8355 - val_loss: 0.3866 - val_accuracy: 0.8400\n",
      "Epoch 365/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3967 - accuracy: 0.8429 - val_loss: 0.3834 - val_accuracy: 0.8410\n",
      "Epoch 366/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3890 - accuracy: 0.8345 - val_loss: 0.3842 - val_accuracy: 0.8400\n",
      "Epoch 367/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4015 - accuracy: 0.8286 - val_loss: 0.3988 - val_accuracy: 0.8152\n",
      "Epoch 368/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4050 - accuracy: 0.8220 - val_loss: 0.3958 - val_accuracy: 0.8190\n",
      "Epoch 369/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3962 - accuracy: 0.8251 - val_loss: 0.3914 - val_accuracy: 0.8267\n",
      "Epoch 370/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3852 - accuracy: 0.8302 - val_loss: 0.3862 - val_accuracy: 0.8333\n",
      "Epoch 371/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3995 - accuracy: 0.8327 - val_loss: 0.3855 - val_accuracy: 0.8362\n",
      "Epoch 372/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3944 - accuracy: 0.8284 - val_loss: 0.3847 - val_accuracy: 0.8390\n",
      "Epoch 373/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3916 - accuracy: 0.8335 - val_loss: 0.3811 - val_accuracy: 0.8381\n",
      "Epoch 374/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8386 - val_loss: 0.3833 - val_accuracy: 0.8381\n",
      "Epoch 375/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.8373 - val_loss: 0.3852 - val_accuracy: 0.8390\n",
      "Epoch 376/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3938 - accuracy: 0.8316 - val_loss: 0.3807 - val_accuracy: 0.8410\n",
      "Epoch 377/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3876 - accuracy: 0.8371 - val_loss: 0.3914 - val_accuracy: 0.8367\n",
      "Epoch 378/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4061 - accuracy: 0.8400 - val_loss: 0.3808 - val_accuracy: 0.8424\n",
      "Epoch 379/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3974 - accuracy: 0.8416 - val_loss: 0.3761 - val_accuracy: 0.8414\n",
      "Epoch 380/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4089 - accuracy: 0.8369 - val_loss: 0.3844 - val_accuracy: 0.8395\n",
      "Epoch 381/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3856 - accuracy: 0.8386 - val_loss: 0.3801 - val_accuracy: 0.8400\n",
      "Epoch 382/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3941 - accuracy: 0.8380 - val_loss: 0.3795 - val_accuracy: 0.8400\n",
      "Epoch 383/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3890 - accuracy: 0.8380 - val_loss: 0.3788 - val_accuracy: 0.8381\n",
      "Epoch 384/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3986 - accuracy: 0.8422 - val_loss: 0.3794 - val_accuracy: 0.8390\n",
      "Epoch 385/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3854 - accuracy: 0.8394 - val_loss: 0.3818 - val_accuracy: 0.8386\n",
      "Epoch 386/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3858 - accuracy: 0.8322 - val_loss: 0.3811 - val_accuracy: 0.8381\n",
      "Epoch 387/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3801 - accuracy: 0.8422 - val_loss: 0.3821 - val_accuracy: 0.8386\n",
      "Epoch 388/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8353 - val_loss: 0.3793 - val_accuracy: 0.8410\n",
      "Epoch 389/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8302 - val_loss: 0.3941 - val_accuracy: 0.8367\n",
      "Epoch 390/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8316 - val_loss: 0.3888 - val_accuracy: 0.8357\n",
      "Epoch 391/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8386 - val_loss: 0.3855 - val_accuracy: 0.8410\n",
      "Epoch 392/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8396 - val_loss: 0.3816 - val_accuracy: 0.8405\n",
      "Epoch 393/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3825 - accuracy: 0.8380 - val_loss: 0.3826 - val_accuracy: 0.8410\n",
      "Epoch 394/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8333 - val_loss: 0.3781 - val_accuracy: 0.8419\n",
      "Epoch 395/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3907 - accuracy: 0.8382 - val_loss: 0.4001 - val_accuracy: 0.8367\n",
      "Epoch 396/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3986 - accuracy: 0.8343 - val_loss: 0.3935 - val_accuracy: 0.8405\n",
      "Epoch 397/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8398 - val_loss: 0.3818 - val_accuracy: 0.8419\n",
      "Epoch 398/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.3840 - accuracy: 0.8396 - val_loss: 0.4125 - val_accuracy: 0.8167\n",
      "Epoch 399/3500\n",
      "154/154 [==============================] - 0s 1ms/step - loss: 0.4009 - accuracy: 0.8210 - val_loss: 0.4027 - val_accuracy: 0.8276\n",
      "Epoch 399: early stopping\n"
     ]
    }
   ],
   "source": [
    "store = s_model.fit(x_train , y_train , epochs=3500 , validation_split = 0.3 , callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed4510fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3.5677490234375,\n",
       "  3.3278286457061768,\n",
       "  3.0601956844329834,\n",
       "  2.947988510131836,\n",
       "  2.776369094848633,\n",
       "  2.79604172706604,\n",
       "  2.812084913253784,\n",
       "  2.742098808288574,\n",
       "  2.6959939002990723,\n",
       "  2.53796648979187,\n",
       "  2.481462240219116,\n",
       "  2.3340108394622803,\n",
       "  2.4499671459198,\n",
       "  2.242642402648926,\n",
       "  2.302675724029541,\n",
       "  2.211693048477173,\n",
       "  2.098317861557007,\n",
       "  2.1109237670898438,\n",
       "  2.0427486896514893,\n",
       "  1.9143173694610596,\n",
       "  1.8568259477615356,\n",
       "  1.885770320892334,\n",
       "  1.7927913665771484,\n",
       "  1.8527992963790894,\n",
       "  1.7640889883041382,\n",
       "  1.7078256607055664,\n",
       "  1.8039456605911255,\n",
       "  1.6745548248291016,\n",
       "  1.6623750925064087,\n",
       "  1.5946235656738281,\n",
       "  1.605790376663208,\n",
       "  1.6177095174789429,\n",
       "  1.5464550256729126,\n",
       "  1.6314127445220947,\n",
       "  1.5988353490829468,\n",
       "  1.4943486452102661,\n",
       "  1.5217348337173462,\n",
       "  1.5250933170318604,\n",
       "  1.4513088464736938,\n",
       "  1.4080127477645874,\n",
       "  1.3852895498275757,\n",
       "  1.27898371219635,\n",
       "  1.2856227159500122,\n",
       "  1.2981399297714233,\n",
       "  1.2836687564849854,\n",
       "  1.2737153768539429,\n",
       "  1.2301175594329834,\n",
       "  1.2639780044555664,\n",
       "  1.2486387491226196,\n",
       "  1.2010481357574463,\n",
       "  1.187799096107483,\n",
       "  1.18944251537323,\n",
       "  1.1276025772094727,\n",
       "  1.1170169115066528,\n",
       "  1.1686749458312988,\n",
       "  1.0744242668151855,\n",
       "  1.0726191997528076,\n",
       "  1.0608527660369873,\n",
       "  1.047315001487732,\n",
       "  1.068937063217163,\n",
       "  1.0480231046676636,\n",
       "  1.0118411779403687,\n",
       "  0.9838131070137024,\n",
       "  1.0229387283325195,\n",
       "  1.0021709203720093,\n",
       "  1.0125439167022705,\n",
       "  0.9747207760810852,\n",
       "  1.0008162260055542,\n",
       "  0.9200546145439148,\n",
       "  0.9725622534751892,\n",
       "  0.8953447937965393,\n",
       "  0.9327939748764038,\n",
       "  0.9335692524909973,\n",
       "  0.897998571395874,\n",
       "  0.9066731929779053,\n",
       "  0.9199521541595459,\n",
       "  0.8488593101501465,\n",
       "  0.8381146192550659,\n",
       "  0.8778481483459473,\n",
       "  0.8157163858413696,\n",
       "  0.843149721622467,\n",
       "  0.8383376598358154,\n",
       "  0.8371291756629944,\n",
       "  0.8463719487190247,\n",
       "  0.8421820998191833,\n",
       "  0.8410619497299194,\n",
       "  0.8411584496498108,\n",
       "  0.8112422227859497,\n",
       "  0.7983734011650085,\n",
       "  0.8060224056243896,\n",
       "  0.8112882375717163,\n",
       "  0.7983083128929138,\n",
       "  0.7850966453552246,\n",
       "  0.7744506597518921,\n",
       "  0.7556449174880981,\n",
       "  0.7669142484664917,\n",
       "  0.708456814289093,\n",
       "  0.7352272272109985,\n",
       "  0.7019557952880859,\n",
       "  0.7000385522842407,\n",
       "  0.7287365198135376,\n",
       "  0.7236450910568237,\n",
       "  0.6679229736328125,\n",
       "  0.6800224184989929,\n",
       "  0.653019905090332,\n",
       "  0.6647042632102966,\n",
       "  0.6383122801780701,\n",
       "  0.6639077663421631,\n",
       "  0.6208214163780212,\n",
       "  0.6099861264228821,\n",
       "  0.6095852851867676,\n",
       "  0.5943038463592529,\n",
       "  0.6067513227462769,\n",
       "  0.5890859961509705,\n",
       "  0.6179366707801819,\n",
       "  0.5802346467971802,\n",
       "  0.6144546270370483,\n",
       "  0.5950531959533691,\n",
       "  0.5772451758384705,\n",
       "  0.5884859561920166,\n",
       "  0.557874321937561,\n",
       "  0.5853527784347534,\n",
       "  0.5769900679588318,\n",
       "  0.5814950466156006,\n",
       "  0.5664042830467224,\n",
       "  0.5730299949645996,\n",
       "  0.5593316555023193,\n",
       "  0.5712044835090637,\n",
       "  0.5653112530708313,\n",
       "  0.5749152898788452,\n",
       "  0.5688225030899048,\n",
       "  0.5653067231178284,\n",
       "  0.5460562109947205,\n",
       "  0.5526196956634521,\n",
       "  0.5644034743309021,\n",
       "  0.5493566393852234,\n",
       "  0.5427298545837402,\n",
       "  0.5312103629112244,\n",
       "  0.5231473445892334,\n",
       "  0.5477964282035828,\n",
       "  0.5315542221069336,\n",
       "  0.5593236684799194,\n",
       "  0.5202957391738892,\n",
       "  0.5191059708595276,\n",
       "  0.5319003462791443,\n",
       "  0.5070569515228271,\n",
       "  0.5273726582527161,\n",
       "  0.5480955243110657,\n",
       "  0.5450909733772278,\n",
       "  0.5313987135887146,\n",
       "  0.528651773929596,\n",
       "  0.5024142265319824,\n",
       "  0.5129324197769165,\n",
       "  0.5284413695335388,\n",
       "  0.5127454400062561,\n",
       "  0.5111033916473389,\n",
       "  0.5069225430488586,\n",
       "  0.5011683702468872,\n",
       "  0.5083252191543579,\n",
       "  0.4952406883239746,\n",
       "  0.5126684308052063,\n",
       "  0.501357913017273,\n",
       "  0.5085588097572327,\n",
       "  0.4983472228050232,\n",
       "  0.49679338932037354,\n",
       "  0.5113261938095093,\n",
       "  0.5148508548736572,\n",
       "  0.5013769268989563,\n",
       "  0.5094911456108093,\n",
       "  0.49362194538116455,\n",
       "  0.49424439668655396,\n",
       "  0.48475781083106995,\n",
       "  0.49753788113594055,\n",
       "  0.5036009550094604,\n",
       "  0.5001239776611328,\n",
       "  0.49116456508636475,\n",
       "  0.4903387129306793,\n",
       "  0.4783843457698822,\n",
       "  0.4916745126247406,\n",
       "  0.48003697395324707,\n",
       "  0.4806463122367859,\n",
       "  0.4968312680721283,\n",
       "  0.48204779624938965,\n",
       "  0.4625808298587799,\n",
       "  0.4702758193016052,\n",
       "  0.4857209622859955,\n",
       "  0.46828076243400574,\n",
       "  0.4663327634334564,\n",
       "  0.48790326714515686,\n",
       "  0.4590730667114258,\n",
       "  0.4655478298664093,\n",
       "  0.4687790870666504,\n",
       "  0.4641546607017517,\n",
       "  0.4763351082801819,\n",
       "  0.47984549403190613,\n",
       "  0.48400574922561646,\n",
       "  0.46244189143180847,\n",
       "  0.4636041522026062,\n",
       "  0.46359190344810486,\n",
       "  0.4660666584968567,\n",
       "  0.459734171628952,\n",
       "  0.46028152108192444,\n",
       "  0.46322259306907654,\n",
       "  0.4671034812927246,\n",
       "  0.46050742268562317,\n",
       "  0.46455806493759155,\n",
       "  0.4590981602668762,\n",
       "  0.46059590578079224,\n",
       "  0.4716373383998871,\n",
       "  0.4618557095527649,\n",
       "  0.45208051800727844,\n",
       "  0.45185306668281555,\n",
       "  0.46698063611984253,\n",
       "  0.4644639492034912,\n",
       "  0.45817625522613525,\n",
       "  0.44525688886642456,\n",
       "  0.45099350810050964,\n",
       "  0.45993706583976746,\n",
       "  0.4535909593105316,\n",
       "  0.46320751309394836,\n",
       "  0.4528965651988983,\n",
       "  0.4521820545196533,\n",
       "  0.44734713435173035,\n",
       "  0.45291832089424133,\n",
       "  0.4436834454536438,\n",
       "  0.44477832317352295,\n",
       "  0.4418555200099945,\n",
       "  0.43936288356781006,\n",
       "  0.4508517384529114,\n",
       "  0.4419713616371155,\n",
       "  0.43625229597091675,\n",
       "  0.44535157084465027,\n",
       "  0.44282281398773193,\n",
       "  0.4377247989177704,\n",
       "  0.4431406557559967,\n",
       "  0.43562808632850647,\n",
       "  0.4408356547355652,\n",
       "  0.44845739006996155,\n",
       "  0.4415602684020996,\n",
       "  0.4385133683681488,\n",
       "  0.43860307335853577,\n",
       "  0.4343714118003845,\n",
       "  0.4361204504966736,\n",
       "  0.4275784194469452,\n",
       "  0.4335614740848541,\n",
       "  0.4254312217235565,\n",
       "  0.42106708884239197,\n",
       "  0.4234476685523987,\n",
       "  0.4294096827507019,\n",
       "  0.4354197680950165,\n",
       "  0.4207260012626648,\n",
       "  0.4238421618938446,\n",
       "  0.4278905689716339,\n",
       "  0.42210474610328674,\n",
       "  0.4288157820701599,\n",
       "  0.42868831753730774,\n",
       "  0.41212278604507446,\n",
       "  0.4207056164741516,\n",
       "  0.42131516337394714,\n",
       "  0.41932618618011475,\n",
       "  0.413980096578598,\n",
       "  0.4161287844181061,\n",
       "  0.40832510590553284,\n",
       "  0.4225076735019684,\n",
       "  0.4088474214076996,\n",
       "  0.41401344537734985,\n",
       "  0.41378724575042725,\n",
       "  0.4064016044139862,\n",
       "  0.41604799032211304,\n",
       "  0.410207599401474,\n",
       "  0.41815435886383057,\n",
       "  0.422320693731308,\n",
       "  0.4068971574306488,\n",
       "  0.40762007236480713,\n",
       "  0.41060957312583923,\n",
       "  0.3956453502178192,\n",
       "  0.4161751866340637,\n",
       "  0.398608535528183,\n",
       "  0.41504961252212524,\n",
       "  0.39888790249824524,\n",
       "  0.4138798713684082,\n",
       "  0.403841108083725,\n",
       "  0.39820367097854614,\n",
       "  0.4156978130340576,\n",
       "  0.40868887305259705,\n",
       "  0.40528860688209534,\n",
       "  0.4063636362552643,\n",
       "  0.4138689935207367,\n",
       "  0.4124484658241272,\n",
       "  0.3971271216869354,\n",
       "  0.4032132029533386,\n",
       "  0.4162500500679016,\n",
       "  0.43205222487449646,\n",
       "  0.42204177379608154,\n",
       "  0.4184158742427826,\n",
       "  0.4104037880897522,\n",
       "  0.4103067219257355,\n",
       "  0.40780842304229736,\n",
       "  0.40690475702285767,\n",
       "  0.40747398138046265,\n",
       "  0.4123289883136749,\n",
       "  0.4002114534378052,\n",
       "  0.4217960834503174,\n",
       "  0.40859776735305786,\n",
       "  0.4011634290218353,\n",
       "  0.39732086658477783,\n",
       "  0.4104282855987549,\n",
       "  0.4000872075557709,\n",
       "  0.400677353143692,\n",
       "  0.4128895699977875,\n",
       "  0.400738924741745,\n",
       "  0.4028010666370392,\n",
       "  0.4215099811553955,\n",
       "  0.4107583463191986,\n",
       "  0.412973552942276,\n",
       "  0.40955421328544617,\n",
       "  0.4088813066482544,\n",
       "  0.4134371876716614,\n",
       "  0.4034040868282318,\n",
       "  0.3964445888996124,\n",
       "  0.38691580295562744,\n",
       "  0.4011325240135193,\n",
       "  0.4035360515117645,\n",
       "  0.3858160078525543,\n",
       "  0.39039650559425354,\n",
       "  0.41461455821990967,\n",
       "  0.4016799330711365,\n",
       "  0.4070166051387787,\n",
       "  0.3990092873573303,\n",
       "  0.39342358708381653,\n",
       "  0.40749451518058777,\n",
       "  0.3944801986217499,\n",
       "  0.3924349248409271,\n",
       "  0.4220786392688751,\n",
       "  0.39551201462745667,\n",
       "  0.38836145401000977,\n",
       "  0.41545459628105164,\n",
       "  0.39501217007637024,\n",
       "  0.39943790435791016,\n",
       "  0.3966592252254486,\n",
       "  0.390299528837204,\n",
       "  0.4002315104007721,\n",
       "  0.38659048080444336,\n",
       "  0.4004116952419281,\n",
       "  0.415245920419693,\n",
       "  0.4092628061771393,\n",
       "  0.3910810053348541,\n",
       "  0.4054868221282959,\n",
       "  0.4025851786136627,\n",
       "  0.3886699676513672,\n",
       "  0.3957103192806244,\n",
       "  0.399323433637619,\n",
       "  0.3940452039241791,\n",
       "  0.3918934762477875,\n",
       "  0.3898453116416931,\n",
       "  0.39352694153785706,\n",
       "  0.385636568069458,\n",
       "  0.3958089351654053,\n",
       "  0.390564888715744,\n",
       "  0.40352052450180054,\n",
       "  0.4082048535346985,\n",
       "  0.3924122154712677,\n",
       "  0.39956870675086975,\n",
       "  0.3859594762325287,\n",
       "  0.3966934084892273,\n",
       "  0.3889865279197693,\n",
       "  0.40148666501045227,\n",
       "  0.40497374534606934,\n",
       "  0.3962370753288269,\n",
       "  0.3852474093437195,\n",
       "  0.39949434995651245,\n",
       "  0.3944171369075775,\n",
       "  0.3916204273700714,\n",
       "  0.38803553581237793,\n",
       "  0.3937138319015503,\n",
       "  0.39376100897789,\n",
       "  0.3875960111618042,\n",
       "  0.4061198830604553,\n",
       "  0.39744701981544495,\n",
       "  0.4089363217353821,\n",
       "  0.3855718970298767,\n",
       "  0.39407530426979065,\n",
       "  0.38901448249816895,\n",
       "  0.3986000418663025,\n",
       "  0.38542649149894714,\n",
       "  0.385820597410202,\n",
       "  0.38006097078323364,\n",
       "  0.38743287324905396,\n",
       "  0.3980230391025543,\n",
       "  0.40212830901145935,\n",
       "  0.3880321681499481,\n",
       "  0.3905918300151825,\n",
       "  0.38253703713417053,\n",
       "  0.3993014693260193,\n",
       "  0.39071860909461975,\n",
       "  0.3986475467681885,\n",
       "  0.38551586866378784,\n",
       "  0.3840440809726715,\n",
       "  0.40085622668266296],\n",
       " 'accuracy': [0.6514285802841187,\n",
       "  0.6769387722015381,\n",
       "  0.6987755298614502,\n",
       "  0.7110204100608826,\n",
       "  0.7314285635948181,\n",
       "  0.723061203956604,\n",
       "  0.7248979806900024,\n",
       "  0.7208163142204285,\n",
       "  0.7257142663002014,\n",
       "  0.722244918346405,\n",
       "  0.7251020669937134,\n",
       "  0.7340816259384155,\n",
       "  0.7285714149475098,\n",
       "  0.7385714054107666,\n",
       "  0.7320408225059509,\n",
       "  0.7381632924079895,\n",
       "  0.7422448992729187,\n",
       "  0.7275510430335999,\n",
       "  0.7322449088096619,\n",
       "  0.7432653307914734,\n",
       "  0.7467346787452698,\n",
       "  0.7579591870307922,\n",
       "  0.7642857432365417,\n",
       "  0.7530612349510193,\n",
       "  0.7508163452148438,\n",
       "  0.7544897794723511,\n",
       "  0.7512245178222656,\n",
       "  0.7402040958404541,\n",
       "  0.740408182144165,\n",
       "  0.7397959232330322,\n",
       "  0.7359183430671692,\n",
       "  0.7324489951133728,\n",
       "  0.7371428608894348,\n",
       "  0.7342857122421265,\n",
       "  0.7773469090461731,\n",
       "  0.7726530432701111,\n",
       "  0.7702040672302246,\n",
       "  0.7699999809265137,\n",
       "  0.7724489569664001,\n",
       "  0.7663265466690063,\n",
       "  0.763469398021698,\n",
       "  0.7542856931686401,\n",
       "  0.7665306329727173,\n",
       "  0.7575510144233704,\n",
       "  0.7616326808929443,\n",
       "  0.7565305829048157,\n",
       "  0.7724489569664001,\n",
       "  0.7614285945892334,\n",
       "  0.7646938562393188,\n",
       "  0.7646938562393188,\n",
       "  0.76816326379776,\n",
       "  0.7530612349510193,\n",
       "  0.7620407938957214,\n",
       "  0.758775532245636,\n",
       "  0.7640816569328308,\n",
       "  0.7671428322792053,\n",
       "  0.7604081630706787,\n",
       "  0.7710204124450684,\n",
       "  0.7726530432701111,\n",
       "  0.7691836953163147,\n",
       "  0.7602040767669678,\n",
       "  0.7481632828712463,\n",
       "  0.7446938753128052,\n",
       "  0.7469387650489807,\n",
       "  0.7473469376564026,\n",
       "  0.7504081726074219,\n",
       "  0.7657142877578735,\n",
       "  0.7742857336997986,\n",
       "  0.7657142877578735,\n",
       "  0.7665306329727173,\n",
       "  0.7704081535339355,\n",
       "  0.7738775610923767,\n",
       "  0.772040843963623,\n",
       "  0.7767347097396851,\n",
       "  0.7761224508285522,\n",
       "  0.777550995349884,\n",
       "  0.7867347002029419,\n",
       "  0.7844898104667664,\n",
       "  0.7830612063407898,\n",
       "  0.7885714173316956,\n",
       "  0.7859183549880981,\n",
       "  0.7873469591140747,\n",
       "  0.7842857241630554,\n",
       "  0.7871428728103638,\n",
       "  0.7906122207641602,\n",
       "  0.78632652759552,\n",
       "  0.7869387865066528,\n",
       "  0.7895918488502502,\n",
       "  0.7916326522827148,\n",
       "  0.7926530838012695,\n",
       "  0.7934693694114685,\n",
       "  0.7989795804023743,\n",
       "  0.7928571701049805,\n",
       "  0.7975510358810425,\n",
       "  0.7963265180587769,\n",
       "  0.7934693694114685,\n",
       "  0.7932652831077576,\n",
       "  0.7977551221847534,\n",
       "  0.7924489974975586,\n",
       "  0.7946938872337341,\n",
       "  0.7983673214912415,\n",
       "  0.7979592084884644,\n",
       "  0.8020408153533936,\n",
       "  0.7936734557151794,\n",
       "  0.8006122708320618,\n",
       "  0.8014285564422607,\n",
       "  0.7981632947921753,\n",
       "  0.7989795804023743,\n",
       "  0.8026530742645264,\n",
       "  0.8008162975311279,\n",
       "  0.8002040982246399,\n",
       "  0.7983673214912415,\n",
       "  0.7977551221847534,\n",
       "  0.8022449016571045,\n",
       "  0.8002040982246399,\n",
       "  0.8034693598747253,\n",
       "  0.80448979139328,\n",
       "  0.8036734461784363,\n",
       "  0.8073469400405884,\n",
       "  0.8022449016571045,\n",
       "  0.8010203838348389,\n",
       "  0.8022449016571045,\n",
       "  0.8034693598747253,\n",
       "  0.8028571605682373,\n",
       "  0.8008162975311279,\n",
       "  0.8075510263442993,\n",
       "  0.8048979640007019,\n",
       "  0.80448979139328,\n",
       "  0.8108163475990295,\n",
       "  0.8055102229118347,\n",
       "  0.809387743473053,\n",
       "  0.8012244701385498,\n",
       "  0.8077551126480103,\n",
       "  0.809387743473053,\n",
       "  0.8040816187858582,\n",
       "  0.8073469400405884,\n",
       "  0.8020408153533936,\n",
       "  0.8083673715591431,\n",
       "  0.8038775324821472,\n",
       "  0.80448979139328,\n",
       "  0.8079591989517212,\n",
       "  0.8071428537368774,\n",
       "  0.8083673715591431,\n",
       "  0.8081632852554321,\n",
       "  0.8042857050895691,\n",
       "  0.8089795708656311,\n",
       "  0.8073469400405884,\n",
       "  0.8089795708656311,\n",
       "  0.8071428537368774,\n",
       "  0.8089795708656311,\n",
       "  0.8067346811294556,\n",
       "  0.804693877696991,\n",
       "  0.8057143092155457,\n",
       "  0.8077551126480103,\n",
       "  0.7995918393135071,\n",
       "  0.8030612468719482,\n",
       "  0.8114285469055176,\n",
       "  0.8081632852554321,\n",
       "  0.8148979544639587,\n",
       "  0.8061224222183228,\n",
       "  0.7971428632736206,\n",
       "  0.7973469495773315,\n",
       "  0.8036734461784363,\n",
       "  0.8051020503044128,\n",
       "  0.8100000023841858,\n",
       "  0.8059183955192566,\n",
       "  0.8065305948257446,\n",
       "  0.8059183955192566,\n",
       "  0.8083673715591431,\n",
       "  0.8061224222183228,\n",
       "  0.8108163475990295,\n",
       "  0.809387743473053,\n",
       "  0.8114285469055176,\n",
       "  0.808571457862854,\n",
       "  0.8122448921203613,\n",
       "  0.8095918297767639,\n",
       "  0.808571457862854,\n",
       "  0.813469409942627,\n",
       "  0.8077551126480103,\n",
       "  0.8130612373352051,\n",
       "  0.8124489784240723,\n",
       "  0.8114285469055176,\n",
       "  0.8130612373352051,\n",
       "  0.8126530647277832,\n",
       "  0.8077551126480103,\n",
       "  0.8077551126480103,\n",
       "  0.8075510263442993,\n",
       "  0.8071428537368774,\n",
       "  0.8079591989517212,\n",
       "  0.8136734962463379,\n",
       "  0.8130612373352051,\n",
       "  0.8161224722862244,\n",
       "  0.8151020407676697,\n",
       "  0.814081609249115,\n",
       "  0.8126530647277832,\n",
       "  0.8120408058166504,\n",
       "  0.8081632852554321,\n",
       "  0.8118367195129395,\n",
       "  0.8116326332092285,\n",
       "  0.8116326332092285,\n",
       "  0.8171428442001343,\n",
       "  0.8157142996788025,\n",
       "  0.813877522945404,\n",
       "  0.813265323638916,\n",
       "  0.8104081749916077,\n",
       "  0.8165305852890015,\n",
       "  0.817959189414978,\n",
       "  0.8175510168075562,\n",
       "  0.8144897818565369,\n",
       "  0.8153061270713806,\n",
       "  0.8161224722862244,\n",
       "  0.8136734962463379,\n",
       "  0.8124489784240723,\n",
       "  0.8100000023841858,\n",
       "  0.8100000023841858,\n",
       "  0.8128571510314941,\n",
       "  0.8173469305038452,\n",
       "  0.8148979544639587,\n",
       "  0.8185714483261108,\n",
       "  0.8136734962463379,\n",
       "  0.813265323638916,\n",
       "  0.8122448921203613,\n",
       "  0.8130612373352051,\n",
       "  0.8185714483261108,\n",
       "  0.813877522945404,\n",
       "  0.8151020407676697,\n",
       "  0.8155102133750916,\n",
       "  0.8193877339363098,\n",
       "  0.8130612373352051,\n",
       "  0.8165305852890015,\n",
       "  0.8197959065437317,\n",
       "  0.8144897818565369,\n",
       "  0.814081609249115,\n",
       "  0.8173469305038452,\n",
       "  0.8232653141021729,\n",
       "  0.817959189414978,\n",
       "  0.8232653141021729,\n",
       "  0.8195918202400208,\n",
       "  0.8173469305038452,\n",
       "  0.8195918202400208,\n",
       "  0.8199999928474426,\n",
       "  0.8193877339363098,\n",
       "  0.8128571510314941,\n",
       "  0.8210204243659973,\n",
       "  0.8097959160804749,\n",
       "  0.8165305852890015,\n",
       "  0.8189796209335327,\n",
       "  0.8148979544639587,\n",
       "  0.8269387483596802,\n",
       "  0.8183673620223999,\n",
       "  0.8244897723197937,\n",
       "  0.8234694004058838,\n",
       "  0.8240816593170166,\n",
       "  0.8212245106697083,\n",
       "  0.8265306353569031,\n",
       "  0.8191836476325989,\n",
       "  0.8224489688873291,\n",
       "  0.8257142901420593,\n",
       "  0.8255102038383484,\n",
       "  0.8246938586235046,\n",
       "  0.8289796113967896,\n",
       "  0.8255102038383484,\n",
       "  0.8240816593170166,\n",
       "  0.8248979449272156,\n",
       "  0.8306122422218323,\n",
       "  0.8220407962799072,\n",
       "  0.8279591798782349,\n",
       "  0.831428587436676,\n",
       "  0.8299999833106995,\n",
       "  0.8240816593170166,\n",
       "  0.8285714387893677,\n",
       "  0.8224489688873291,\n",
       "  0.8210204243659973,\n",
       "  0.8238775730133057,\n",
       "  0.827551007270813,\n",
       "  0.8326530456542969,\n",
       "  0.8293877840042114,\n",
       "  0.8342857360839844,\n",
       "  0.827346920967102,\n",
       "  0.836122453212738,\n",
       "  0.8251020312309265,\n",
       "  0.832244873046875,\n",
       "  0.8293877840042114,\n",
       "  0.8336734771728516,\n",
       "  0.8310204148292542,\n",
       "  0.8257142901420593,\n",
       "  0.8308163285255432,\n",
       "  0.8259183764457703,\n",
       "  0.8287755250930786,\n",
       "  0.8293877840042114,\n",
       "  0.836326539516449,\n",
       "  0.8295918107032776,\n",
       "  0.8167346715927124,\n",
       "  0.8261224627494812,\n",
       "  0.8261224627494812,\n",
       "  0.8265306353569031,\n",
       "  0.8265306353569031,\n",
       "  0.8283673524856567,\n",
       "  0.8310204148292542,\n",
       "  0.8242856860160828,\n",
       "  0.8348979353904724,\n",
       "  0.8342857360839844,\n",
       "  0.8285714387893677,\n",
       "  0.8389796018600464,\n",
       "  0.8389796018600464,\n",
       "  0.8371428847312927,\n",
       "  0.8265306353569031,\n",
       "  0.8324489593505859,\n",
       "  0.8299999833106995,\n",
       "  0.8291836977005005,\n",
       "  0.8308163285255432,\n",
       "  0.8308163285255432,\n",
       "  0.8285714387893677,\n",
       "  0.8324489593505859,\n",
       "  0.8330612182617188,\n",
       "  0.8346938490867615,\n",
       "  0.8279591798782349,\n",
       "  0.8295918107032776,\n",
       "  0.8383673429489136,\n",
       "  0.8289796113967896,\n",
       "  0.8357142806053162,\n",
       "  0.8377550840377808,\n",
       "  0.8310204148292542,\n",
       "  0.8308163285255432,\n",
       "  0.8342857360839844,\n",
       "  0.831428587436676,\n",
       "  0.8340816497802734,\n",
       "  0.8312245011329651,\n",
       "  0.8283673524856567,\n",
       "  0.8336734771728516,\n",
       "  0.831428587436676,\n",
       "  0.8281632661819458,\n",
       "  0.8299999833106995,\n",
       "  0.8310204148292542,\n",
       "  0.8312245011329651,\n",
       "  0.8304081559181213,\n",
       "  0.8257142901420593,\n",
       "  0.8295918107032776,\n",
       "  0.8299999833106995,\n",
       "  0.8336734771728516,\n",
       "  0.8320407867431641,\n",
       "  0.8263265490531921,\n",
       "  0.8328571319580078,\n",
       "  0.8328571319580078,\n",
       "  0.831632673740387,\n",
       "  0.8330612182617188,\n",
       "  0.8369387984275818,\n",
       "  0.8271428346633911,\n",
       "  0.8291836977005005,\n",
       "  0.8326530456542969,\n",
       "  0.826734721660614,\n",
       "  0.822857141494751,\n",
       "  0.8295918107032776,\n",
       "  0.8357142806053162,\n",
       "  0.8332653045654297,\n",
       "  0.8324489593505859,\n",
       "  0.8351020216941833,\n",
       "  0.8304081559181213,\n",
       "  0.8375509977340698,\n",
       "  0.8346938490867615,\n",
       "  0.8299999833106995,\n",
       "  0.831632673740387,\n",
       "  0.8283673524856567,\n",
       "  0.8355101943016052,\n",
       "  0.8428571224212646,\n",
       "  0.8344898223876953,\n",
       "  0.8285714387893677,\n",
       "  0.8220407962799072,\n",
       "  0.8251020312309265,\n",
       "  0.8302040696144104,\n",
       "  0.8326530456542969,\n",
       "  0.8283673524856567,\n",
       "  0.8334693908691406,\n",
       "  0.8385714292526245,\n",
       "  0.8373469114303589,\n",
       "  0.831632673740387,\n",
       "  0.8371428847312927,\n",
       "  0.8399999737739563,\n",
       "  0.8416326642036438,\n",
       "  0.8369387984275818,\n",
       "  0.8385714292526245,\n",
       "  0.8379591703414917,\n",
       "  0.8379591703414917,\n",
       "  0.8422449231147766,\n",
       "  0.8393877744674683,\n",
       "  0.832244873046875,\n",
       "  0.8422449231147766,\n",
       "  0.8353061079978943,\n",
       "  0.8302040696144104,\n",
       "  0.831632673740387,\n",
       "  0.8385714292526245,\n",
       "  0.8395918607711792,\n",
       "  0.8379591703414917,\n",
       "  0.8332653045654297,\n",
       "  0.8381632566452026,\n",
       "  0.8342857360839844,\n",
       "  0.8397959470748901,\n",
       "  0.8395918607711792,\n",
       "  0.8210204243659973],\n",
       " 'val_loss': [2.5858607292175293,\n",
       "  2.5964725017547607,\n",
       "  2.615751028060913,\n",
       "  2.4976119995117188,\n",
       "  2.488048553466797,\n",
       "  2.4684720039367676,\n",
       "  2.4102530479431152,\n",
       "  2.2302565574645996,\n",
       "  2.1757216453552246,\n",
       "  2.015632390975952,\n",
       "  2.0100557804107666,\n",
       "  1.9903204441070557,\n",
       "  1.9552180767059326,\n",
       "  1.8006612062454224,\n",
       "  1.8106929063796997,\n",
       "  1.7367281913757324,\n",
       "  1.6566104888916016,\n",
       "  1.597497820854187,\n",
       "  1.5585567951202393,\n",
       "  1.489543080329895,\n",
       "  1.3765865564346313,\n",
       "  1.4047858715057373,\n",
       "  1.390340805053711,\n",
       "  1.389945149421692,\n",
       "  1.3687925338745117,\n",
       "  1.329146146774292,\n",
       "  1.3076982498168945,\n",
       "  1.3237042427062988,\n",
       "  1.3267395496368408,\n",
       "  1.3017969131469727,\n",
       "  1.290066123008728,\n",
       "  1.2691680192947388,\n",
       "  1.2957285642623901,\n",
       "  1.2561135292053223,\n",
       "  1.2924847602844238,\n",
       "  1.2850250005722046,\n",
       "  1.284356713294983,\n",
       "  1.2492341995239258,\n",
       "  1.2362416982650757,\n",
       "  1.2075557708740234,\n",
       "  1.184539794921875,\n",
       "  1.1732467412948608,\n",
       "  1.1871788501739502,\n",
       "  1.1968309879302979,\n",
       "  1.15482497215271,\n",
       "  1.1679233312606812,\n",
       "  1.156008243560791,\n",
       "  1.1337462663650513,\n",
       "  1.1450375318527222,\n",
       "  1.125483512878418,\n",
       "  1.1044236421585083,\n",
       "  1.0566564798355103,\n",
       "  1.0412458181381226,\n",
       "  1.0375405550003052,\n",
       "  1.0414848327636719,\n",
       "  1.0224801301956177,\n",
       "  1.0268666744232178,\n",
       "  1.003822922706604,\n",
       "  1.0052937269210815,\n",
       "  0.9999268651008606,\n",
       "  0.9334018230438232,\n",
       "  0.9259713888168335,\n",
       "  0.9277016520500183,\n",
       "  0.9284124374389648,\n",
       "  0.9132326245307922,\n",
       "  0.9250762462615967,\n",
       "  0.9295303821563721,\n",
       "  0.9345691204071045,\n",
       "  0.9278708100318909,\n",
       "  0.9094598889350891,\n",
       "  0.8902115821838379,\n",
       "  0.9093729853630066,\n",
       "  0.891474187374115,\n",
       "  0.8833976984024048,\n",
       "  0.8755532503128052,\n",
       "  0.8628976941108704,\n",
       "  0.844735860824585,\n",
       "  0.8439592123031616,\n",
       "  0.838325560092926,\n",
       "  0.8332109451293945,\n",
       "  0.8279430866241455,\n",
       "  0.8273141980171204,\n",
       "  0.8327697515487671,\n",
       "  0.8201245665550232,\n",
       "  0.816003680229187,\n",
       "  0.8154857754707336,\n",
       "  0.8070428967475891,\n",
       "  0.7816954851150513,\n",
       "  0.7750108242034912,\n",
       "  0.7708770036697388,\n",
       "  0.773273766040802,\n",
       "  0.7666704058647156,\n",
       "  0.7488268613815308,\n",
       "  0.729871928691864,\n",
       "  0.703574538230896,\n",
       "  0.6688073873519897,\n",
       "  0.660803496837616,\n",
       "  0.6473603248596191,\n",
       "  0.6401317715644836,\n",
       "  0.6372538805007935,\n",
       "  0.6365739107131958,\n",
       "  0.627038836479187,\n",
       "  0.6182057857513428,\n",
       "  0.6049595475196838,\n",
       "  0.6092391610145569,\n",
       "  0.6076931953430176,\n",
       "  0.5998916625976562,\n",
       "  0.6034151911735535,\n",
       "  0.5883914828300476,\n",
       "  0.5820924043655396,\n",
       "  0.5804970264434814,\n",
       "  0.5789877772331238,\n",
       "  0.5754809379577637,\n",
       "  0.5780146718025208,\n",
       "  0.5745848417282104,\n",
       "  0.5773118138313293,\n",
       "  0.5558551549911499,\n",
       "  0.5648088455200195,\n",
       "  0.56004399061203,\n",
       "  0.562174916267395,\n",
       "  0.5567617416381836,\n",
       "  0.5528300404548645,\n",
       "  0.5491345524787903,\n",
       "  0.5451043844223022,\n",
       "  0.5430561304092407,\n",
       "  0.5473117232322693,\n",
       "  0.5384241938591003,\n",
       "  0.5371310710906982,\n",
       "  0.5345413088798523,\n",
       "  0.5318472385406494,\n",
       "  0.5313379764556885,\n",
       "  0.530339777469635,\n",
       "  0.5301648378372192,\n",
       "  0.5412817597389221,\n",
       "  0.5269845724105835,\n",
       "  0.5260741710662842,\n",
       "  0.5225239396095276,\n",
       "  0.5225828886032104,\n",
       "  0.5283182263374329,\n",
       "  0.52192223072052,\n",
       "  0.5073996782302856,\n",
       "  0.5091812610626221,\n",
       "  0.5024948120117188,\n",
       "  0.4996708631515503,\n",
       "  0.502638041973114,\n",
       "  0.49515753984451294,\n",
       "  0.5199695825576782,\n",
       "  0.5166472792625427,\n",
       "  0.5124766826629639,\n",
       "  0.5075562596321106,\n",
       "  0.5030984282493591,\n",
       "  0.49686703085899353,\n",
       "  0.49514496326446533,\n",
       "  0.4927489757537842,\n",
       "  0.4903903305530548,\n",
       "  0.48895037174224854,\n",
       "  0.4934626817703247,\n",
       "  0.4897651672363281,\n",
       "  0.4889222979545593,\n",
       "  0.5067058801651001,\n",
       "  0.4998704195022583,\n",
       "  0.4943251311779022,\n",
       "  0.4946676790714264,\n",
       "  0.4852764308452606,\n",
       "  0.48289844393730164,\n",
       "  0.4816688597202301,\n",
       "  0.4792425036430359,\n",
       "  0.47690001130104065,\n",
       "  0.4710123836994171,\n",
       "  0.46993911266326904,\n",
       "  0.4684932827949524,\n",
       "  0.46042197942733765,\n",
       "  0.4660640358924866,\n",
       "  0.4657837450504303,\n",
       "  0.4660487174987793,\n",
       "  0.4508896470069885,\n",
       "  0.45085957646369934,\n",
       "  0.44770553708076477,\n",
       "  0.44663453102111816,\n",
       "  0.44908255338668823,\n",
       "  0.4503919780254364,\n",
       "  0.4534960687160492,\n",
       "  0.4451836049556732,\n",
       "  0.443897008895874,\n",
       "  0.4657405614852905,\n",
       "  0.4597814977169037,\n",
       "  0.45438921451568604,\n",
       "  0.45044079422950745,\n",
       "  0.4499243497848511,\n",
       "  0.4459017813205719,\n",
       "  0.44484806060791016,\n",
       "  0.44557905197143555,\n",
       "  0.4453359842300415,\n",
       "  0.44243237376213074,\n",
       "  0.44446563720703125,\n",
       "  0.4612369239330292,\n",
       "  0.4508019685745239,\n",
       "  0.44690239429473877,\n",
       "  0.4434237778186798,\n",
       "  0.44191819429397583,\n",
       "  0.4385615587234497,\n",
       "  0.45420292019844055,\n",
       "  0.4520190954208374,\n",
       "  0.44557592272758484,\n",
       "  0.44125357270240784,\n",
       "  0.438573956489563,\n",
       "  0.4373098611831665,\n",
       "  0.43834158778190613,\n",
       "  0.4373286962509155,\n",
       "  0.4352235496044159,\n",
       "  0.43891873955726624,\n",
       "  0.437003493309021,\n",
       "  0.462827205657959,\n",
       "  0.4546012878417969,\n",
       "  0.45285308361053467,\n",
       "  0.44770747423171997,\n",
       "  0.44537603855133057,\n",
       "  0.4437045156955719,\n",
       "  0.44112321734428406,\n",
       "  0.44141077995300293,\n",
       "  0.445002943277359,\n",
       "  0.44061049818992615,\n",
       "  0.4379624128341675,\n",
       "  0.43732142448425293,\n",
       "  0.44388410449028015,\n",
       "  0.43793439865112305,\n",
       "  0.44282665848731995,\n",
       "  0.43219661712646484,\n",
       "  0.42668187618255615,\n",
       "  0.4295451045036316,\n",
       "  0.4300086498260498,\n",
       "  0.42655470967292786,\n",
       "  0.4263041615486145,\n",
       "  0.4219803810119629,\n",
       "  0.4211089611053467,\n",
       "  0.42177486419677734,\n",
       "  0.4198762774467468,\n",
       "  0.4183272123336792,\n",
       "  0.4276067018508911,\n",
       "  0.42298486828804016,\n",
       "  0.4191085696220398,\n",
       "  0.4201250970363617,\n",
       "  0.42062580585479736,\n",
       "  0.4299219846725464,\n",
       "  0.4358094334602356,\n",
       "  0.42036139965057373,\n",
       "  0.41671016812324524,\n",
       "  0.422252893447876,\n",
       "  0.41692283749580383,\n",
       "  0.42472681403160095,\n",
       "  0.4155366122722626,\n",
       "  0.41438254714012146,\n",
       "  0.410495787858963,\n",
       "  0.40963125228881836,\n",
       "  0.4073275327682495,\n",
       "  0.4152474105358124,\n",
       "  0.4119504988193512,\n",
       "  0.4075009226799011,\n",
       "  0.4198308289051056,\n",
       "  0.4137483537197113,\n",
       "  0.4076210558414459,\n",
       "  0.4122941195964813,\n",
       "  0.4057707190513611,\n",
       "  0.4071177840232849,\n",
       "  0.40466374158859253,\n",
       "  0.39827781915664673,\n",
       "  0.4035559594631195,\n",
       "  0.39997178316116333,\n",
       "  0.40539830923080444,\n",
       "  0.3964252173900604,\n",
       "  0.39485108852386475,\n",
       "  0.41449102759361267,\n",
       "  0.40013790130615234,\n",
       "  0.40376016497612,\n",
       "  0.39699462056159973,\n",
       "  0.39454832673072815,\n",
       "  0.39245709776878357,\n",
       "  0.3946555256843567,\n",
       "  0.4007083773612976,\n",
       "  0.3975115120410919,\n",
       "  0.4026801586151123,\n",
       "  0.3946891129016876,\n",
       "  0.391926646232605,\n",
       "  0.4025754928588867,\n",
       "  0.4036198556423187,\n",
       "  0.3948202133178711,\n",
       "  0.4028242528438568,\n",
       "  0.41329941153526306,\n",
       "  0.4006979763507843,\n",
       "  0.3919726610183716,\n",
       "  0.4058644771575928,\n",
       "  0.43818339705467224,\n",
       "  0.42754217982292175,\n",
       "  0.4252302944660187,\n",
       "  0.41557255387306213,\n",
       "  0.4104454219341278,\n",
       "  0.4067530333995819,\n",
       "  0.4030439257621765,\n",
       "  0.39971524477005005,\n",
       "  0.39710840582847595,\n",
       "  0.3994757831096649,\n",
       "  0.39758405089378357,\n",
       "  0.39046576619148254,\n",
       "  0.3899531364440918,\n",
       "  0.3870146870613098,\n",
       "  0.3909430205821991,\n",
       "  0.4003154933452606,\n",
       "  0.4012022912502289,\n",
       "  0.3967399597167969,\n",
       "  0.3884236216545105,\n",
       "  0.3900102376937866,\n",
       "  0.38848206400871277,\n",
       "  0.41451066732406616,\n",
       "  0.4033452868461609,\n",
       "  0.39833611249923706,\n",
       "  0.39288604259490967,\n",
       "  0.3931025266647339,\n",
       "  0.3900570273399353,\n",
       "  0.38981398940086365,\n",
       "  0.3890058100223541,\n",
       "  0.38570329546928406,\n",
       "  0.38664257526397705,\n",
       "  0.3963378369808197,\n",
       "  0.39346736669540405,\n",
       "  0.38919728994369507,\n",
       "  0.3914993703365326,\n",
       "  0.39922669529914856,\n",
       "  0.40619879961013794,\n",
       "  0.39364251494407654,\n",
       "  0.38751599192619324,\n",
       "  0.39309102296829224,\n",
       "  0.384345680475235,\n",
       "  0.3951570987701416,\n",
       "  0.3961266577243805,\n",
       "  0.38501474261283875,\n",
       "  0.39406469464302063,\n",
       "  0.3954968750476837,\n",
       "  0.39395180344581604,\n",
       "  0.38831213116645813,\n",
       "  0.38771340250968933,\n",
       "  0.3862149715423584,\n",
       "  0.39627307653427124,\n",
       "  0.38965022563934326,\n",
       "  0.39236754179000854,\n",
       "  0.3897503614425659,\n",
       "  0.3962118327617645,\n",
       "  0.38387930393218994,\n",
       "  0.3889607787132263,\n",
       "  0.3909980356693268,\n",
       "  0.3858218789100647,\n",
       "  0.4085220992565155,\n",
       "  0.3913545310497284,\n",
       "  0.38419297337532043,\n",
       "  0.385168194770813,\n",
       "  0.38283464312553406,\n",
       "  0.38723379373550415,\n",
       "  0.38427454233169556,\n",
       "  0.3826189637184143,\n",
       "  0.3810229003429413,\n",
       "  0.3955499827861786,\n",
       "  0.387813001871109,\n",
       "  0.3899696171283722,\n",
       "  0.39483633637428284,\n",
       "  0.3866165578365326,\n",
       "  0.38337746262550354,\n",
       "  0.3842412829399109,\n",
       "  0.39878585934638977,\n",
       "  0.3958330452442169,\n",
       "  0.3914225697517395,\n",
       "  0.3861939311027527,\n",
       "  0.3855186104774475,\n",
       "  0.38467249274253845,\n",
       "  0.38107722997665405,\n",
       "  0.3832835257053375,\n",
       "  0.3851574659347534,\n",
       "  0.38074207305908203,\n",
       "  0.39142653346061707,\n",
       "  0.38079264760017395,\n",
       "  0.3761104941368103,\n",
       "  0.38441672921180725,\n",
       "  0.3800860047340393,\n",
       "  0.37949222326278687,\n",
       "  0.37879058718681335,\n",
       "  0.37942859530448914,\n",
       "  0.3818318545818329,\n",
       "  0.381092369556427,\n",
       "  0.38205423951148987,\n",
       "  0.3793294131755829,\n",
       "  0.39411723613739014,\n",
       "  0.38876232504844666,\n",
       "  0.38546115159988403,\n",
       "  0.38162848353385925,\n",
       "  0.38258492946624756,\n",
       "  0.3780699372291565,\n",
       "  0.40010374784469604,\n",
       "  0.39350974559783936,\n",
       "  0.3818093538284302,\n",
       "  0.41248828172683716,\n",
       "  0.40272751450538635],\n",
       " 'val_accuracy': [0.7066666483879089,\n",
       "  0.7285714149475098,\n",
       "  0.7476190328598022,\n",
       "  0.7523809671401978,\n",
       "  0.7614285945892334,\n",
       "  0.7676190733909607,\n",
       "  0.7685714364051819,\n",
       "  0.7642857432365417,\n",
       "  0.7671428322792053,\n",
       "  0.7719047665596008,\n",
       "  0.773809552192688,\n",
       "  0.7733333110809326,\n",
       "  0.7733333110809326,\n",
       "  0.7704761624336243,\n",
       "  0.7785714268684387,\n",
       "  0.7814285755157471,\n",
       "  0.7828571200370789,\n",
       "  0.7785714268684387,\n",
       "  0.7838095426559448,\n",
       "  0.7780952453613281,\n",
       "  0.7809523940086365,\n",
       "  0.7823809385299683,\n",
       "  0.7819047570228577,\n",
       "  0.7714285850524902,\n",
       "  0.7742857336997986,\n",
       "  0.7757142782211304,\n",
       "  0.7752380967140198,\n",
       "  0.7671428322792053,\n",
       "  0.7685714364051819,\n",
       "  0.7695237994194031,\n",
       "  0.7723809480667114,\n",
       "  0.772857129573822,\n",
       "  0.7723809480667114,\n",
       "  0.7852380871772766,\n",
       "  0.7861904501914978,\n",
       "  0.7857142686843872,\n",
       "  0.7866666913032532,\n",
       "  0.7871428728103638,\n",
       "  0.7866666913032532,\n",
       "  0.7852380871772766,\n",
       "  0.7861904501914978,\n",
       "  0.7876190543174744,\n",
       "  0.788095235824585,\n",
       "  0.7900000214576721,\n",
       "  0.7895237803459167,\n",
       "  0.7904762029647827,\n",
       "  0.7904762029647827,\n",
       "  0.7909523844718933,\n",
       "  0.7909523844718933,\n",
       "  0.7919047474861145,\n",
       "  0.7895237803459167,\n",
       "  0.7923809289932251,\n",
       "  0.7919047474861145,\n",
       "  0.7952380776405334,\n",
       "  0.7952380776405334,\n",
       "  0.7971428632736206,\n",
       "  0.7952380776405334,\n",
       "  0.7952380776405334,\n",
       "  0.7938095331192017,\n",
       "  0.7938095331192017,\n",
       "  0.7861904501914978,\n",
       "  0.7842857241630554,\n",
       "  0.7871428728103638,\n",
       "  0.7914285659790039,\n",
       "  0.7904762029647827,\n",
       "  0.7919047474861145,\n",
       "  0.7909523844718933,\n",
       "  0.7895237803459167,\n",
       "  0.7895237803459167,\n",
       "  0.7885714173316956,\n",
       "  0.7890475988388062,\n",
       "  0.7852380871772766,\n",
       "  0.7842857241630554,\n",
       "  0.7857142686843872,\n",
       "  0.788095235824585,\n",
       "  0.7895237803459167,\n",
       "  0.7904762029647827,\n",
       "  0.7895237803459167,\n",
       "  0.7895237803459167,\n",
       "  0.7914285659790039,\n",
       "  0.7914285659790039,\n",
       "  0.7919047474861145,\n",
       "  0.7914285659790039,\n",
       "  0.7904762029647827,\n",
       "  0.7914285659790039,\n",
       "  0.7890475988388062,\n",
       "  0.7890475988388062,\n",
       "  0.7895237803459167,\n",
       "  0.7885714173316956,\n",
       "  0.7885714173316956,\n",
       "  0.7885714173316956,\n",
       "  0.7885714173316956,\n",
       "  0.7885714173316956,\n",
       "  0.7895237803459167,\n",
       "  0.7919047474861145,\n",
       "  0.7928571701049805,\n",
       "  0.7933333516120911,\n",
       "  0.7928571701049805,\n",
       "  0.7928571701049805,\n",
       "  0.7919047474861145,\n",
       "  0.7928571701049805,\n",
       "  0.7919047474861145,\n",
       "  0.7938095331192017,\n",
       "  0.7923809289932251,\n",
       "  0.7928571701049805,\n",
       "  0.7919047474861145,\n",
       "  0.7914285659790039,\n",
       "  0.7914285659790039,\n",
       "  0.7914285659790039,\n",
       "  0.7938095331192017,\n",
       "  0.7919047474861145,\n",
       "  0.7919047474861145,\n",
       "  0.7938095331192017,\n",
       "  0.7914285659790039,\n",
       "  0.7933333516120911,\n",
       "  0.7914285659790039,\n",
       "  0.7919047474861145,\n",
       "  0.7928571701049805,\n",
       "  0.7952380776405334,\n",
       "  0.7947618961334229,\n",
       "  0.7952380776405334,\n",
       "  0.7938095331192017,\n",
       "  0.7942857146263123,\n",
       "  0.7938095331192017,\n",
       "  0.7933333516120911,\n",
       "  0.7942857146263123,\n",
       "  0.79666668176651,\n",
       "  0.7947618961334229,\n",
       "  0.7961905002593994,\n",
       "  0.7971428632736206,\n",
       "  0.7980952262878418,\n",
       "  0.7985714077949524,\n",
       "  0.799047589302063,\n",
       "  0.7976190447807312,\n",
       "  0.7995238304138184,\n",
       "  0.800000011920929,\n",
       "  0.8009523749351501,\n",
       "  0.8014285564422607,\n",
       "  0.7942857146263123,\n",
       "  0.7971428632736206,\n",
       "  0.799047589302063,\n",
       "  0.7995238304138184,\n",
       "  0.795714259147644,\n",
       "  0.7971428632736206,\n",
       "  0.795714259147644,\n",
       "  0.7995238304138184,\n",
       "  0.7952380776405334,\n",
       "  0.79666668176651,\n",
       "  0.7971428632736206,\n",
       "  0.7971428632736206,\n",
       "  0.7980952262878418,\n",
       "  0.7976190447807312,\n",
       "  0.7976190447807312,\n",
       "  0.799047589302063,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.8004761934280396,\n",
       "  0.8019047379493713,\n",
       "  0.799047589302063,\n",
       "  0.788095235824585,\n",
       "  0.7876190543174744,\n",
       "  0.7942857146263123,\n",
       "  0.7952380776405334,\n",
       "  0.795714259147644,\n",
       "  0.7980952262878418,\n",
       "  0.7976190447807312,\n",
       "  0.8014285564422607,\n",
       "  0.8019047379493713,\n",
       "  0.8004761934280396,\n",
       "  0.800000011920929,\n",
       "  0.8014285564422607,\n",
       "  0.8019047379493713,\n",
       "  0.8014285564422607,\n",
       "  0.8023809790611267,\n",
       "  0.8047618865966797,\n",
       "  0.8042857050895691,\n",
       "  0.8042857050895691,\n",
       "  0.8061904907226562,\n",
       "  0.8052380681037903,\n",
       "  0.8042857050895691,\n",
       "  0.8061904907226562,\n",
       "  0.8123809695243835,\n",
       "  0.8142856955528259,\n",
       "  0.8104761838912964,\n",
       "  0.8014285564422607,\n",
       "  0.8052380681037903,\n",
       "  0.8042857050895691,\n",
       "  0.8052380681037903,\n",
       "  0.8066666722297668,\n",
       "  0.8100000023841858,\n",
       "  0.8104761838912964,\n",
       "  0.8095238208770752,\n",
       "  0.810952365398407,\n",
       "  0.8138095140457153,\n",
       "  0.8090476393699646,\n",
       "  0.79666668176651,\n",
       "  0.8052380681037903,\n",
       "  0.8066666722297668,\n",
       "  0.8104761838912964,\n",
       "  0.8104761838912964,\n",
       "  0.810952365398407,\n",
       "  0.8157142996788025,\n",
       "  0.8090476393699646,\n",
       "  0.8180952668190002,\n",
       "  0.8180952668190002,\n",
       "  0.8185714483261108,\n",
       "  0.8171428442001343,\n",
       "  0.811904788017273,\n",
       "  0.8147618770599365,\n",
       "  0.8161904811859131,\n",
       "  0.810952365398407,\n",
       "  0.8138095140457153,\n",
       "  0.7976190447807312,\n",
       "  0.8009523749351501,\n",
       "  0.808571457862854,\n",
       "  0.808571457862854,\n",
       "  0.8104761838912964,\n",
       "  0.810952365398407,\n",
       "  0.8133333325386047,\n",
       "  0.8128571510314941,\n",
       "  0.8100000023841858,\n",
       "  0.8147618770599365,\n",
       "  0.8147618770599365,\n",
       "  0.8157142996788025,\n",
       "  0.8052380681037903,\n",
       "  0.8114285469055176,\n",
       "  0.8061904907226562,\n",
       "  0.8152381181716919,\n",
       "  0.8190476298332214,\n",
       "  0.8176190257072449,\n",
       "  0.8180952668190002,\n",
       "  0.819523811340332,\n",
       "  0.8176190257072449,\n",
       "  0.8209523558616638,\n",
       "  0.8247619271278381,\n",
       "  0.8252381086349487,\n",
       "  0.8280952572822571,\n",
       "  0.8257142901420593,\n",
       "  0.8276190757751465,\n",
       "  0.8247619271278381,\n",
       "  0.8257142901420593,\n",
       "  0.8233333230018616,\n",
       "  0.8266666531562805,\n",
       "  0.811904788017273,\n",
       "  0.8100000023841858,\n",
       "  0.822857141494751,\n",
       "  0.822857141494751,\n",
       "  0.8209523558616638,\n",
       "  0.8252381086349487,\n",
       "  0.819523811340332,\n",
       "  0.8242856860160828,\n",
       "  0.8290476202964783,\n",
       "  0.8323809504508972,\n",
       "  0.831428587436676,\n",
       "  0.8328571319580078,\n",
       "  0.8295238018035889,\n",
       "  0.8333333134651184,\n",
       "  0.8319047689437866,\n",
       "  0.8252381086349487,\n",
       "  0.8285714387893677,\n",
       "  0.8304761648178101,\n",
       "  0.8328571319580078,\n",
       "  0.8304761648178101,\n",
       "  0.8290476202964783,\n",
       "  0.831428587436676,\n",
       "  0.8342857360839844,\n",
       "  0.831428587436676,\n",
       "  0.8390476107597351,\n",
       "  0.8261904716491699,\n",
       "  0.8342857360839844,\n",
       "  0.8361904621124268,\n",
       "  0.8176190257072449,\n",
       "  0.8299999833106995,\n",
       "  0.8323809504508972,\n",
       "  0.8357142806053162,\n",
       "  0.8361904621124268,\n",
       "  0.8376190662384033,\n",
       "  0.8371428847312927,\n",
       "  0.8376190662384033,\n",
       "  0.8361904621124268,\n",
       "  0.8342857360839844,\n",
       "  0.8390476107597351,\n",
       "  0.8380952477455139,\n",
       "  0.8342857360839844,\n",
       "  0.8309524059295654,\n",
       "  0.8371428847312927,\n",
       "  0.8323809504508972,\n",
       "  0.8242856860160828,\n",
       "  0.8328571319580078,\n",
       "  0.8371428847312927,\n",
       "  0.8357142806053162,\n",
       "  0.8052380681037903,\n",
       "  0.8257142901420593,\n",
       "  0.8299999833106995,\n",
       "  0.8295238018035889,\n",
       "  0.8323809504508972,\n",
       "  0.8352380990982056,\n",
       "  0.8342857360839844,\n",
       "  0.8333333134651184,\n",
       "  0.8366666436195374,\n",
       "  0.8376190662384033,\n",
       "  0.8385714292526245,\n",
       "  0.8390476107597351,\n",
       "  0.8376190662384033,\n",
       "  0.8395237922668457,\n",
       "  0.8342857360839844,\n",
       "  0.8333333134651184,\n",
       "  0.8357142806053162,\n",
       "  0.8371428847312927,\n",
       "  0.8390476107597351,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8299999833106995,\n",
       "  0.8376190662384033,\n",
       "  0.8409523963928223,\n",
       "  0.8409523963928223,\n",
       "  0.8371428847312927,\n",
       "  0.8409523963928223,\n",
       "  0.8376190662384033,\n",
       "  0.8361904621124268,\n",
       "  0.8380952477455139,\n",
       "  0.8371428847312927,\n",
       "  0.8385714292526245,\n",
       "  0.8399999737739563,\n",
       "  0.8395237922668457,\n",
       "  0.8395237922668457,\n",
       "  0.8385714292526245,\n",
       "  0.8304761648178101,\n",
       "  0.8395237922668457,\n",
       "  0.8414285778999329,\n",
       "  0.8395237922668457,\n",
       "  0.8428571224212646,\n",
       "  0.8409523963928223,\n",
       "  0.8395237922668457,\n",
       "  0.8399999737739563,\n",
       "  0.8409523963928223,\n",
       "  0.8261904716491699,\n",
       "  0.8280952572822571,\n",
       "  0.8376190662384033,\n",
       "  0.8380952477455139,\n",
       "  0.8404762148857117,\n",
       "  0.8342857360839844,\n",
       "  0.8395237922668457,\n",
       "  0.8380952477455139,\n",
       "  0.8428571224212646,\n",
       "  0.8366666436195374,\n",
       "  0.8433333039283752,\n",
       "  0.8371428847312927,\n",
       "  0.8361904621124268,\n",
       "  0.8366666436195374,\n",
       "  0.8185714483261108,\n",
       "  0.8357142806053162,\n",
       "  0.8404762148857117,\n",
       "  0.8404762148857117,\n",
       "  0.8409523963928223,\n",
       "  0.8409523963928223,\n",
       "  0.8404762148857117,\n",
       "  0.8390476107597351,\n",
       "  0.8404762148857117,\n",
       "  0.8342857360839844,\n",
       "  0.8371428847312927,\n",
       "  0.8376190662384033,\n",
       "  0.8352380990982056,\n",
       "  0.8399999737739563,\n",
       "  0.8409523963928223,\n",
       "  0.8399999737739563,\n",
       "  0.8152381181716919,\n",
       "  0.8190476298332214,\n",
       "  0.8266666531562805,\n",
       "  0.8333333134651184,\n",
       "  0.8361904621124268,\n",
       "  0.8390476107597351,\n",
       "  0.8380952477455139,\n",
       "  0.8380952477455139,\n",
       "  0.8390476107597351,\n",
       "  0.8409523963928223,\n",
       "  0.8366666436195374,\n",
       "  0.842380940914154,\n",
       "  0.8414285778999329,\n",
       "  0.8395237922668457,\n",
       "  0.8399999737739563,\n",
       "  0.8399999737739563,\n",
       "  0.8380952477455139,\n",
       "  0.8390476107597351,\n",
       "  0.8385714292526245,\n",
       "  0.8380952477455139,\n",
       "  0.8385714292526245,\n",
       "  0.8409523963928223,\n",
       "  0.8366666436195374,\n",
       "  0.8357142806053162,\n",
       "  0.8409523963928223,\n",
       "  0.8404762148857117,\n",
       "  0.8409523963928223,\n",
       "  0.8419047594070435,\n",
       "  0.8366666436195374,\n",
       "  0.8404762148857117,\n",
       "  0.8419047594070435,\n",
       "  0.8166666626930237,\n",
       "  0.8276190757751465]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42916fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 678us/step\n"
     ]
    }
   ],
   "source": [
    "y_float = s_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7befa46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11815594],\n",
       "       [0.07301031],\n",
       "       [0.05811001],\n",
       "       ...,\n",
       "       [0.08072335],\n",
       "       [0.07145631],\n",
       "       [0.0488039 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6854cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_float>=0.5 , 1 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fa480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
